{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== [Loading Dataset] ==================================================\n",
      "================================================== [Loading Model] ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/sae_lens/sae.py:151: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\"\"\"\n",
    "TODO(Adriano) after getting some plots for the blobs post-SAE in GPT2, it's important to check whether these\n",
    "SAEs are actually any good. Unfortunately, I have really bad FVUs, MSEs, etc... It's also unclear if a error\n",
    "norm of 30 is normal. People seem not to be reporting this very well and it's deeply annoying.\n",
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import dotenv\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "class SAEExtractor:\n",
    "    \"\"\"\n",
    "    Wraps a HookedSAETransformer and a bank of pre-trained SAEs to combine\n",
    "    (1) loading the SAEs from the repository using SAELens, (2) Running them in\n",
    "    a hooked transformer, getting activations, etc...\n",
    "\n",
    "    This is meant ONLY for gpt2 and JBloom's SAEs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        device: str | torch.device | None = None,\n",
    "    ) -> None:\n",
    "        self.model_name = \"gpt2\"\n",
    "        self.sae_release = \"gpt2-small-res-jb\"\n",
    "        self.device = \"cuda\" # NOTE: you should use CUDA_VISIBLE_DEVICES to select the GPU\n",
    "        self.model = HookedSAETransformer.from_pretrained(self.model_name, device=self.device)\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        # self.saes: Dict[str, SAE] = {}\n",
    "        self._load_saes()\n",
    "\n",
    "    def _load_saes(self) -> None:\n",
    "        # TODO(Adriano) make this more modular...\n",
    "        self.block2sae = []\n",
    "        self.cfg_dics = []\n",
    "        self.sparsities = []\n",
    "        # print(self.model.cfg.n_layers)\n",
    "        for layer in range(self.model.cfg.n_layers):\n",
    "            sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "                release = self.sae_release, # see other options in sae_lens/pretrained_saes.yaml\n",
    "                sae_id = f\"blocks.{layer}.hook_resid_pre\", # won't always be a hook point\n",
    "                device = self.device\n",
    "            )\n",
    "            self.block2sae.append(sae)\n",
    "            self.cfg_dics.append(cfg_dict)\n",
    "            self.sparsities.append(sparsity)\n",
    "\n",
    "    def cache_activations(self, tokens: torch.Tensor, hooks: List[str] | None = None) -> Dict[str, torch.Tensor]:\n",
    "        hooks = hooks or list(self.saes)\n",
    "        _, cache = self.model.run_with_cache_with_saes(tokens, act_names=hooks)\n",
    "        out: Dict[str, torch.Tensor] = {}\n",
    "        for hook in hooks:\n",
    "            out_key = f\"{hook}.hook_sae_acts_post\"\n",
    "            out[out_key] = cache[out_key].cpu()\n",
    "        return out\n",
    "\n",
    "print(\"=\"*50 + \" [Loading Dataset] \" + \"=\"*50) # DEBUG\n",
    "# dataset = load_dataset(\"openwebtext\", split=\"train\", trust_remote_code=True)\n",
    "dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\", trust_remote_code=True) # Smaller version\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=1024, #sae.cfg.context_size,\n",
    "    add_bos_token=True, #sae.cfg.prepend_bos,\n",
    ")\n",
    "# print(token_dataset) # Sanity\n",
    "# print(token_dataset[0]['tokens']) # Sanity\n",
    "\n",
    "print(\"=\"*50 + \" [Loading Model] \" + \"=\"*50) # DEBUG\n",
    "extractor = SAEExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 300, 1024, 768])\n",
      "torch.Size([12, 300, 1024, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 30: 100%|██████████| 10/10 [00:32<00:00,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "token_dataset_short = token_dataset[:300]['tokens']\n",
    "dataset_length = token_dataset_short.shape[0]\n",
    "sequence_length = token_dataset_short.shape[1]\n",
    "sae_ins = torch.zeros((len(extractor.block2sae), dataset_length, sequence_length, extractor.model.cfg.d_model), device=\"cpu\")\n",
    "sae_outs = torch.zeros((len(extractor.block2sae), dataset_length, sequence_length, extractor.model.cfg.d_model), device=\"cpu\")\n",
    "print(sae_ins.shape)\n",
    "print(sae_outs.shape)\n",
    "for i in range(len(extractor.block2sae)):\n",
    "    extractor.block2sae[i].eval()\n",
    "\n",
    "batch_size = 30\n",
    "while True:\n",
    "    try:\n",
    "        pbar = tqdm.trange(0, len(token_dataset_short), batch_size, desc=f\"Batch Size = {batch_size}\")\n",
    "        with torch.no_grad():\n",
    "            for i in pbar:\n",
    "                j = min(i + batch_size, len(token_dataset_short))\n",
    "                # activation store can give us tokens.\n",
    "                batch_tokens = token_dataset_short[i:j]\n",
    "                _, cache = extractor.model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "                # print(cache.keys())\n",
    "                # Use the SAE\n",
    "                # print(len(extractor.block2sae))\n",
    "                # print(f\"hook_name={extractor.block2sae[8].cfg.hook_name}\") # Nope\n",
    "                sae_in = torch.stack([cache[extractor.block2sae[i].cfg.hook_name].detach() for i in range(len(extractor.block2sae))])\n",
    "                del cache\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                # feature_acts = [extractor.block2sae[i].encode(sae_in[i])\n",
    "                sae_out = torch.stack([extractor.block2sae[i](sae_in[i]).detach().cpu() for i in range(len(extractor.block2sae))])\n",
    "\n",
    "                assert sae_in.shape == sae_out.shape\n",
    "                assert sae_in.shape[0] == len(extractor.block2sae)\n",
    "                assert sae_in.shape[1] == j - i\n",
    "                assert sae_in.shape[2] == sequence_length\n",
    "                assert sae_in.shape[3] == extractor.model.cfg.d_model, f\"sae_in.shape={sae_in.shape}, need [2] = {extractor.model.cfg.d_model}\" # fmt: skip\n",
    "                assert sae_in.ndim == 4\n",
    "                sae_ins[:, i:j, :] = sae_in.cpu()\n",
    "                sae_outs[:, i:j, :] = sae_out.cpu()\n",
    "        break # OK\n",
    "    except torch.OutOfMemoryError as e:\n",
    "        print(type(e), e)\n",
    "        if batch_size == 1:\n",
    "            raise e\n",
    "        batch_size = min(1, batch_size // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "#         # l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "#         for i in range(len(extractor.block2sae)):\n",
    "#             err_norm = (sae_out[i] - sae_in[i]).norm(dim=-1).detach()\n",
    "#             fvu = ((sae_out[i] - sae_in[i]).pow(2).sum(dim=-1) / sae_in[i].pow(2).sum(dim=-1))\n",
    "#             mse = (sae_out[i] - sae_in[i]).pow(2).mean(dim=-1).detach() # \"variance explained\"\n",
    "#             # TODO(Adriano) error norm seems REALLY BAD\n",
    "#             print(f\"average error norm: {err_norm.mean().item()} +/- {err_norm.std().item()}; min={err_norm.min().item()}, max={err_norm.max().item()}\")\n",
    "#             print(f\"average mse: {mse.mean().item()} +/- {mse.std().item()}; min={mse.min().item()}, max={mse.max().item()}\")\n",
    "#             print(f\"average fvu: {fvu.mean().item()} +/- {fvu.std().item()}; min={fvu.min().item()}, max={fvu.max().item()}\")\n",
    "#             plt.hist(err_norm.flatten().cpu().log10().numpy(), bins=100) # around 100-1000 error => maybe around 0.7-7 error? => around 1.2 for this shit\n",
    "#             plt.show()\n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening\n",
      "Computing explained variances\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "torch.Size([30720])\n",
      "====================================================================================================\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "torch.Size([12])\n",
      "====================================================================================================\n",
      "Explained variance: 0.9251 +/- 0.1782; min=-0.7076, max=1.0000\n",
      "Explained variance: -18.3155 +/- 56.2406; min=-1238.9437, max=1.0000\n",
      "Explained variance: -5626.2036 +/- 18073.5391; min=-263304.5938, max=1.0000\n",
      "Explained variance: -7083.8779 +/- 21524.2480; min=-304812.6250, max=0.9997\n",
      "Explained variance: -1295.5112 +/- 5120.8442; min=-90918.4766, max=0.9999\n",
      "Explained variance: -307.4859 +/- 1080.4110; min=-28341.6680, max=1.0000\n",
      "Explained variance: -160.1618 +/- 345.7065; min=-6152.4971, max=0.9999\n",
      "Explained variance: -138.6607 +/- 262.8350; min=-6145.1611, max=0.9999\n",
      "Explained variance: -51.0636 +/- 80.9067; min=-1168.5859, max=0.9999\n",
      "Explained variance: -22.2877 +/- 35.5232; min=-768.6276, max=0.9999\n",
      "Explained variance: -6.5486 +/- 13.0736; min=-376.7701, max=0.9963\n",
      "Explained variance: -0.5628 +/- 2.6299; min=-98.5691, max=1.0000\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "print(\"Flattening\")\n",
    "\n",
    "flatten = \"layer batch seq dim -> layer (batch seq) dim\"\n",
    "sae_in_flat = einops.rearrange(sae_in, flatten).cpu()\n",
    "sae_out_flat = einops.rearrange(sae_out, flatten).cpu()\n",
    "\n",
    "print(\"Computing explained variances\")\n",
    "# https://github.com/jbloomAus/SAELens/blob/be0e55f69d360a0100027de1cf3f1a1606cf5552/sae_lens/evals.py#L512\n",
    "sq_errs = (sae_in_flat - sae_out_flat).pow(2).sum(dim=-1) # norms of errors\n",
    "var = (sae_in_flat - sae_in_flat.mean(dim=0)).pow(2).sum(dim=-1) # variance of 1st dim then sum those\n",
    "explained_variances = 1 - sq_errs / var\n",
    "print(\"\\n\".join(map(lambda x: str(x.shape), explained_variances)))\n",
    "# Across batch and sequence unique per layer\n",
    "print(\"=\"*100)\n",
    "mean_evs = explained_variances.mean(dim=-1)\n",
    "std_evs = explained_variances.std(dim=-1)\n",
    "max_evs = explained_variances.max(dim=-1).values\n",
    "min_evs = explained_variances.min(dim=-1).values\n",
    "print(mean_evs.shape)\n",
    "print(std_evs.shape)\n",
    "print(max_evs.shape)\n",
    "print(min_evs.shape)\n",
    "print(\"=\"*100)\n",
    "for mean, std, _max, _min in zip(\n",
    "    mean_evs, std_evs, max_evs, min_evs\n",
    "):\n",
    "    print(f\"Explained variance: {mean.item():.4f} +/- {std.item():.4f}; min={_min.item():.4f}, max={_max.item():.4f}\")\n",
    "\n",
    "# # Plot the explained variances for each layer\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.errorbar(\n",
    "#     range(len(mean_evs)), \n",
    "#     mean_evs, \n",
    "#     yerr=std_evs, \n",
    "#     fmt='o-', \n",
    "#     capsize=5, \n",
    "#     label='Mean Explained Variance with Std Dev'\n",
    "# )\n",
    "# plt.fill_between(\n",
    "#     range(len(mean_evs)),\n",
    "#     min_evs,\n",
    "#     max_evs,\n",
    "#     alpha=0.2,\n",
    "#     label='Min-Max Range'\n",
    "# )\n",
    "# plt.xlabel('Layer')\n",
    "# plt.ylabel('Explained Variance')\n",
    "# plt.title('Explained Variance by Layer')\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Print summary statistics\n",
    "# print(f\"\\nOverall mean explained variance: {sum(mean_evs)/len(mean_evs):.4f}\")\n",
    "# print(f\"Best layer: {torch.argmax(torch.Tensor(mean_evs)).item()} with {max(mean_evs):.4f}\")\n",
    "# print(f\"Worst layer: {torch.argmin(torch.Tensor(mean_evs)).item()} with {min(mean_evs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformer_lens import utils\n",
    "# from functools import partial\n",
    "# model = extractor.model\n",
    "# sae = extractor.block2sae[8]\n",
    "\n",
    "# # next we want to do a reconstruction test.\n",
    "# def reconstr_hook(activation, hook, sae_out):\n",
    "#     return sae_out\n",
    "\n",
    "\n",
    "# def zero_abl_hook(activation, hook):\n",
    "#     return torch.zeros_like(activation)\n",
    "\n",
    "\n",
    "# print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
    "# print(\n",
    "#     \"reconstr\",\n",
    "#     model.run_with_hooks(\n",
    "#         batch_tokens,\n",
    "#         fwd_hooks=[\n",
    "#             (\n",
    "#                 sae.cfg.hook_name,\n",
    "#                 partial(reconstr_hook, sae_out=sae_out[8]),\n",
    "#             )\n",
    "#         ],\n",
    "#         return_type=\"loss\",\n",
    "#     ).item(),\n",
    "# )\n",
    "# print(\n",
    "#     \"Zero\",\n",
    "#     model.run_with_hooks(\n",
    "#         batch_tokens,\n",
    "#         return_type=\"loss\",\n",
    "#         fwd_hooks=[(sae.cfg.hook_name, zero_abl_hook)],\n",
    "#     ).item(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_prompt = \"When John and Mary went to the shops, John gave the bag to\"\n",
    "# example_answer = \" Mary\"\n",
    "# utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "\n",
    "# logits, cache = model.run_with_cache(example_prompt, prepend_bos=True)\n",
    "# tokens = model.to_tokens(example_prompt)\n",
    "# sae_out = sae(cache[sae.cfg.hook_name])\n",
    "\n",
    "\n",
    "# def reconstr_hook(activations, hook, sae_out):\n",
    "#     return sae_out\n",
    "\n",
    "\n",
    "# def zero_abl_hook(mlp_out, hook):\n",
    "#     return torch.zeros_like(mlp_out)\n",
    "\n",
    "\n",
    "# hook_name = sae.cfg.hook_name\n",
    "\n",
    "# print(\"Orig\", model(tokens, return_type=\"loss\").item())\n",
    "# print(\n",
    "#     \"reconstr\",\n",
    "#     model.run_with_hooks(\n",
    "#         tokens,\n",
    "#         fwd_hooks=[\n",
    "#             (\n",
    "#                 hook_name,\n",
    "#                 partial(reconstr_hook, sae_out=sae_out),\n",
    "#             )\n",
    "#         ],\n",
    "#         return_type=\"loss\",\n",
    "#     ).item(),\n",
    "# )\n",
    "# print(\n",
    "#     \"Zero\",\n",
    "#     model.run_with_hooks(\n",
    "#         tokens,\n",
    "#         return_type=\"loss\",\n",
    "#         fwd_hooks=[(hook_name, zero_abl_hook)],\n",
    "#     ).item(),\n",
    "# )\n",
    "\n",
    "\n",
    "# with model.hooks(\n",
    "#     fwd_hooks=[\n",
    "#         (\n",
    "#             hook_name,\n",
    "#             partial(reconstr_hook, sae_out=sae_out),\n",
    "#         )\n",
    "#     ]\n",
    "# ):\n",
    "#     utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sae_dashboard.sae_vis_data import SaeVisConfig\n",
    "# from sae_dashboard.sae_vis_runner import SaeVisRunner\n",
    "# device = \"cuda\"\n",
    "# test_feature_idx_gpt = list(range(10)) + [14057]\n",
    "\n",
    "# feature_vis_config_gpt = SaeVisConfig(\n",
    "#     hook_point=hook_name,\n",
    "#     features=test_feature_idx_gpt,\n",
    "#     minibatch_size_features=64,\n",
    "#     minibatch_size_tokens=256,\n",
    "#     verbose=True,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "# visualization_data_gpt = SaeVisRunner(\n",
    "#     feature_vis_config_gpt\n",
    "# ).run(\n",
    "#     encoder=sae,  # type: ignore\n",
    "#     model=model,\n",
    "#     tokens=token_dataset[:10000][\"tokens\"],  # type: ignore\n",
    "# )\n",
    "# # SaeVisData.create(\n",
    "# #     encoder=sae,\n",
    "# #     model=model, # type: ignore\n",
    "# #     tokens=token_dataset[:10000][\"tokens\"],  # type: ignore\n",
    "# #     cfg=feature_vis_config_gpt,\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sae_dashboard.data_writing_fns import save_feature_centric_vis\n",
    "\n",
    "# filename = f\"demo_feature_dashboards.html\"\n",
    "# save_feature_centric_vis(sae_vis_data=visualization_data_gpt, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "# # this function should open\n",
    "# neuronpedia_quick_list = get_neuronpedia_quick_list(sae, test_feature_idx_gpt)\n",
    "\n",
    "# print(neuronpedia_quick_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-density",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
