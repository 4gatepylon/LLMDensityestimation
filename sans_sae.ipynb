{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\"\"\"\n",
    "TODO(Adriano) after getting some plots for the blobs post-SAE in GPT2, it's important to check whether these\n",
    "SAEs are actually any good. Unfortunately, I have really bad FVUs, MSEs, etc... It's also unclear if a error\n",
    "norm of 30 is normal. People seem not to be reporting this very well and it's deeply annoying.\n",
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import dotenv\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "import tqdm\n",
    "import torch\n",
    "import gc\n",
    "import itertools\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pydantic\n",
    "from transformer_lens import ActivationCache\n",
    "from transformer_lens.components import TransformerBlock, LayerNormPre\n",
    "import shutil\n",
    "\n",
    "# Load our own imports, etc...\n",
    "from sans_sae_lib.utils import plot_cosine_kernel, plot_all_nc2_top_pcs, plot_all_nc2_top_pcs_errs\n",
    "from sans_sae_lib.schemas import ExtractedActivations, FlattenedExtractedActivations\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "assert \"CUDA_VISIBLE_DEVICES\" in os.environ, \"CUDA_VISIBLE_DEVICES is not set\"\n",
    "assert len(os.environ[\"CUDA_VISIBLE_DEVICES\"].strip()) > 0, \"CUDA_VISIBLE_DEVICES is empty\"\n",
    "\n",
    "print(\"=\"*50 + \" [Loading Dataset] \" + \"=\"*50) # DEBUG\n",
    "# dataset = load_dataset(\"openwebtext\", split=\"train\", trust_remote_code=True)\n",
    "dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\", trust_remote_code=True) # Smaller version\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    # NOTE: all these have context 128\n",
    "    max_length=128, #sae.cfg.context_size,\n",
    "    add_bos_token=True, #sae.cfg.prepend_bos,\n",
    ")\n",
    "print(\"=\"*50 + \" [Loading Model] \" + \"=\"*50) # DEBUG\n",
    "# TODO(Adriano) do this below...\n",
    "# for d in extractor.cfg_dics:\n",
    "#     print(d[\"context_size\"]) # NOTE: you should picke the smallest of these...\n",
    "\n",
    "\n",
    "# Shorten the dataset for testing more quickly\n",
    "dataset_size = 300 # XXX make this longer please\n",
    "token_dataset_short = token_dataset[:dataset_size]['tokens']\n",
    "dataset_length = token_dataset_short.shape[0]\n",
    "sequence_length = token_dataset_short.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== [Loading Model] ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/sae_lens/sae.py:151: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== [Extracting Activations] ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 150: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it]\n"
     ]
    }
   ],
   "source": [
    "class ResidAndLn2Comparer:\n",
    "    \"\"\"\n",
    "    A class that basically automates the task of taking in a dataset, tokenizing it, running it through\n",
    "    GPT2 using HookedSAETransformer with JBloom's SAEs.\n",
    "\n",
    "    It is meant for measuring/plotting the impacts on the activations on the model.\n",
    "    The outputs of this module are/should be:\n",
    "    1. Plots of histograms for PCA applied on\n",
    "        - The residual stream activations\n",
    "        - The ln2 activations\n",
    "        - The SAE processed residual stream activations\n",
    "        - The SAE processed ln2 activations\n",
    "        (there are 2D histograms for any pair of PCs in the top 10 PCs)\n",
    "    2. Plots of the MSE, FVU, etc... for the SAEs at each location (including also at ln2) as 1D histograms\n",
    "    3. Plots of the cosine similarity between the PCs pre and post-sae intervention (a 2D heatmap)\n",
    "        (this also will include a )\n",
    "    4. Plots of the cosine similarity between input datapoints and output datapoints pre and post-SAE\n",
    "        intervention in the form of a 1D histogram.\n",
    "    5. SAE error norm/mse/fvu (or in the future any general function)\n",
    "    6. Plots for each K out of a set of top ks of the distribution of reconstructed activations\n",
    "        (this is critical). To do this with non-top-k-trained SAEs we just force the latents to be top-K'ed\n",
    "        but ideally we should also support some top-k SAEs.\n",
    "    \n",
    "    TODO(Adriano) later we want to see how the distributions change as we keep applying SAEs.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Load the model and set some basic settins\n",
    "        self.model_name = \"gpt2\"\n",
    "        self.sae_release = \"gpt2-small-res-jb\"\n",
    "        self.device = \"cuda\" # NOTE: you should use CUDA_VISIBLE_DEVICES to select the GPU\n",
    "        self.model = HookedSAETransformer.from_pretrained(self.model_name, device=self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Load the SAEs and set helper variables\n",
    "        self.d_model = self.model.cfg.d_model\n",
    "        self.n_layers = self.model.cfg.n_layers\n",
    "        self.load_jbloom_gpt2_saes(sae_release=self.sae_release)\n",
    "\n",
    "    def load_jbloom_gpt2_saes(self, sae_release: str = \"gpt2-small-res-jb\"):\n",
    "        self.saes = []\n",
    "        self.sae_cfg_dicts = []\n",
    "        self.sae_sparsities = []\n",
    "        for layer in range(self.model.cfg.n_layers):\n",
    "            sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "                release = sae_release,\n",
    "                sae_id = f\"blocks.{layer}.hook_resid_pre\",\n",
    "                device = self.device\n",
    "            )\n",
    "            sae.eval()\n",
    "            self.saes.append(sae)\n",
    "            self.sae_cfg_dicts.append(cfg_dict)\n",
    "            self.sae_sparsities.append(sparsity)\n",
    "\n",
    "    def get_post_ln2_hookpoint_after_hookpoint(self, hookpoint: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Look here to see where mlp_in happens:\n",
    "        ------------------------------------------------------------\n",
    "        pre:\n",
    "        https://github.com/TransformerLensOrg/TransformerLens/blob/e65fafb4791c66076bc54ec9731920de1e8c676f/transformer_lens/components/transformer_block.py#L191\n",
    "\n",
    "        also\n",
    "        ------------------------------------------------------------\n",
    "        post:\n",
    "        https://github.com/TransformerLensOrg/TransformerLens/blob/e65fafb4791c66076bc54ec9731920de1e8c676f/transformer_lens/components/layer_norm_pre.py#L52\n",
    "        \"\"\"\n",
    "        assert re.match(r\"^blocks.[0-9]+.hook_resid_pre$\", hookpoint)\n",
    "        layer = int(re.match(r\"^blocks.([0-9]+)\\.hook_resid_pre$\", hookpoint).group(1))\n",
    "        return f\"blocks.{layer}.ln2.hook_normalized\"\n",
    "\n",
    "    def get_hookpoint_layer(self, hookpoint: str) -> int:\n",
    "        assert re.match(r\"^blocks.[0-9]+\\..*$\", hookpoint)\n",
    "        return int(re.match(r\"^blocks.([0-9]+)\\..*$\", hookpoint).group(1))\n",
    "\n",
    "    def get_post_ln2_value_after_hookpoint_from_cache(\n",
    "            self,\n",
    "            cache: ActivationCache,\n",
    "            hookpoint: str\n",
    "        ) -> Float[torch.Tensor, \"layer batch seq d_model\"]:\n",
    "        \"\"\"\n",
    "        Quick helper meethod to get the post-ln2 value after a hookpoint in the model (vanilla)\n",
    "        This is used for the express purpose of being able to calculate the SAE's MSE/FVU impact on a\n",
    "        cerain later location (right after ln2 right before MLP).\n",
    "        \"\"\"\n",
    "        # 1. Get the ln2 that we will have to apply\n",
    "        layer_num: int = self.get_hookpoint_layer(hookpoint)\n",
    "        block: TransformerBlock = self.model.blocks[layer_num]\n",
    "        assert isinstance(block, TransformerBlock), f\"block is {type(block)}\"\n",
    "        ln2 = block.ln2\n",
    "        assert isinstance(ln2, LayerNormPre), f\"ln2 is {type(ln2)}\"\n",
    "        # 2. Get the activations\n",
    "        post_ln2_hp: str = self.get_post_ln2_hookpoint_after_hookpoint(hookpoint)\n",
    "        assert post_ln2_hp in cache.keys(), f\"post_ln2_hp={post_ln2_hp} not in cache.keys()={cache.keys()}\"\n",
    "        post_ln2_act: torch.Tensor = cache[post_ln2_hp]\n",
    "        return post_ln2_act\n",
    "    \n",
    "    def get_post_ln2_value_after_hookpoint_from_activations(\n",
    "            self,\n",
    "            activations: Float[torch.Tensor, \"batch seq d_model\"],\n",
    "            hookpoint: str\n",
    "    ) -> Float[torch.Tensor, \"layer batch seq d_model\"]:\n",
    "        \"\"\"\n",
    "        Like `get_post_ln2_value_after_hookpoint` but this will instead take in activations\n",
    "        instead of a cache object. The point is that we will propagate forwards the SAE-processed\n",
    "        activations instead of the vanilla activations.\n",
    "        \"\"\"\n",
    "        # 1. Get the ln2 that we will have to apply\n",
    "        layer_num: int = self.get_hookpoint_layer(hookpoint)\n",
    "        block: TransformerBlock = self.model.blocks[layer_num]\n",
    "        assert isinstance(block, TransformerBlock), f\"block is {type(block)}\"\n",
    "        ln2 = block.ln2\n",
    "        assert isinstance(ln2, LayerNormPre), f\"ln2 is {type(ln2)}\"\n",
    "        # 2. run the model for exactly one layer and hook out the spot right before ln2\n",
    "        debug_number = torch.randn(1, device=self.device) * 999999\n",
    "        buffer_post_ln2 = torch.ones_like(activations) * debug_number\n",
    "        def write_hook(normalized_resid, hook):\n",
    "            buffer_post_ln2[:] = normalized_resid\n",
    "            return normalized_resid\n",
    "        self.model.run_with_hooks(\n",
    "            activations,\n",
    "            fwd_hooks=[\n",
    "                (\n",
    "                    f\"blocks.{layer_num}.ln2.hook_normalized\",\n",
    "                    write_hook,\n",
    "                ),\n",
    "            ],\n",
    "            start_at_layer=layer_num,\n",
    "            stop_at_layer=layer_num+1,\n",
    "        )\n",
    "        # Sanity check that we actually wrote out\n",
    "        assert not torch.any(buffer_post_ln2 == debug_number), f\"buffer_post_ln2 is still {debug_number}\"\n",
    "        return buffer_post_ln2\n",
    "        \n",
    "        \n",
    "    def extract_activations(self, tokens: Int[torch.Tensor, \"batch seq\"], batch_size: int = 30) -> ExtractedActivations:\n",
    "        \"\"\"\n",
    "        Extracts the activations of the model and the SAEs and returns them as tensors.\n",
    "        \"\"\"\n",
    "        # 1. Define output buffers\n",
    "        debug_numbers = torch.randn(4, device=\"cpu\") * 999999\n",
    "        sae_ins = torch.ones((len(self.saes), dataset_length, sequence_length, self.d_model), device=\"cpu\") * debug_numbers[0] # fmt: skip\n",
    "        sae_outs = torch.ones((len(self.saes), dataset_length, sequence_length, self.d_model), device=\"cpu\") * debug_numbers[1] # fmt: skip\n",
    "        ln2s = torch.ones((len(self.saes), dataset_length, sequence_length, self.d_model), device=\"cpu\") * debug_numbers[2] # fmt: skip\n",
    "        ln2s_saed = torch.ones((len(self.saes), dataset_length, sequence_length, self.d_model), device=\"cpu\") * debug_numbers[3] # fmt: skip\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                pbar = tqdm.trange(0, len(tokens), batch_size, desc=f\"Batch Size = {batch_size}\")\n",
    "                with torch.no_grad():\n",
    "                    for i in pbar:\n",
    "                        j = min(i + batch_size, len(tokens))\n",
    "                        # activation store can give us tokens.\n",
    "                        # 1. Get the activations for our current batch of tokens\n",
    "                        # TODO(Adriano) if you do this with run_with_hooks (etc...) you could\n",
    "                        # get better performance for sure.\n",
    "                        batch_tokens = tokens[i:j]\n",
    "                        _, cache = self.model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "\n",
    "                        # 2. Extract the desired activations from the cache\n",
    "                        # print(cache.keys())\n",
    "                        # Use the SAE\n",
    "                        # print(len(extractor.block2sae))\n",
    "                        # print(f\"hook_name={extractor.block2sae[8].cfg.hook_name}\") # Nope\n",
    "                        sae_in = torch.stack([cache[self.saes[i].cfg.hook_name].detach() for i in range(len(self.saes))])\n",
    "                        ln2_ = torch.stack([self.get_post_ln2_value_after_hookpoint_from_cache(cache, self.saes[i].cfg.hook_name) for i in range(len(self.saes))]) # fmt: skip\n",
    "                        del cache\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        assert sae_in.shape == ln2_.shape, f\"sae_in.shape={sae_in.shape}, ln2_.shape={ln2_.shape}\"\n",
    "                        # feature_acts = [extractor.block2sae[i].encode(sae_in[i])\n",
    "                        # TODO(Adriano) this should be possible to parallelize\n",
    "                        sae_out = torch.stack([self.saes[i](sae_in[i]).detach()for i in range(len(self.saes))])\n",
    "                        assert sae_in.shape == sae_out.shape, f\"sae_in.shape={sae_in.shape}, sae_out.shape={sae_out.shape}\"\n",
    "                        ln2_saed = torch.stack([self.get_post_ln2_value_after_hookpoint_from_activations(sae_out[i], self.saes[i].cfg.hook_name) for i in range(len(self.saes))]) # fmt: skip\n",
    "\n",
    "                        # 2. Sanity check the sizes\n",
    "                        assert sae_in.shape == sae_out.shape\n",
    "                        assert sae_in.shape == ln2_.shape # NOTE: this will not scale to different layers but eh\n",
    "                        assert sae_in.shape == ln2_saed.shape\n",
    "                        assert sae_in.shape[0] == len(self.saes)\n",
    "                        assert sae_in.shape[1] == j - i\n",
    "                        assert sae_in.shape[2] == sequence_length\n",
    "                        assert sae_in.shape[3] == self.d_model, f\"sae_in.shape={sae_in.shape}, need [2] = {self.d_model}\" # fmt: skip\n",
    "                        assert sae_in.ndim == 4\n",
    "\n",
    "                        # 3. Store the activations to the appropriate buffers\n",
    "                        sae_ins[:, i:j, :] = sae_in.cpu()\n",
    "                        sae_outs[:, i:j, :] = sae_out.cpu()\n",
    "                        ln2s[:, i:j, :] = ln2_.cpu()\n",
    "                        ln2s_saed[:, i:j, :] = ln2_saed.cpu()\n",
    "                \n",
    "                # Sanity check that we actually wrote out\n",
    "                assert not torch.any(sae_ins == debug_numbers[0]), f\"sae_ins is still {debug_numbers[0]}\"\n",
    "                assert not torch.any(sae_outs == debug_numbers[1]), f\"sae_outs is still {debug_numbers[1]}\"\n",
    "                assert not torch.any(ln2s == debug_numbers[2]), f\"ln2s is still {debug_numbers[2]}\"\n",
    "                assert not torch.any(ln2s_saed == debug_numbers[3]), f\"ln2s_saed is still {debug_numbers[3]}\"\n",
    "                \n",
    "                # Done\n",
    "                return ExtractedActivations(\n",
    "                    sae_ins=sae_ins,\n",
    "                    sae_outs=sae_outs,\n",
    "                    ln2s=ln2s,\n",
    "                    ln2s_saed=ln2s_saed\n",
    "                )\n",
    "            except torch.OutOfMemoryError as e:\n",
    "                print(type(e), e)\n",
    "                if batch_size == 1:\n",
    "                    raise e\n",
    "                batch_size = min(1, batch_size // 2)\n",
    "                pbar.update(desc=f\"Batch Size = {batch_size}\")\n",
    "\n",
    "print(\"=\"*50 + \" [Loading Model] \" + \"=\"*50) # DEBUG\n",
    "comparer = ResidAndLn2Comparer()\n",
    "# print(comparer.model) # DEBUG\n",
    "\n",
    "print(\"=\"*50 + \" [Extracting Activations] \" + \"=\"*50) # DEBUG\n",
    "extracted_activations: ExtractedActivations = comparer.extract_activations(\n",
    "    token_dataset_short,\n",
    "    batch_size=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== [Flattening + Calculating PCA & Errors] ==================================================\n",
      "================================================== [Done, now ready to plot (next cell)] ==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50 + \" [Flattening + Calculating PCA & Errors] \" + \"=\"*50) # DEBUG\n",
    "extracted_activations_flattened: FlattenedExtractedActivations = extracted_activations.flatten()\n",
    "print(\"=\"*50 + \" [Done, now ready to plot (next cell)] \" + \"=\"*50) # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the global plots folder for us to be able to plot everything\"\"\"\n",
    "global_plot_folder_path = Path(\"sae_sans_plots\")\n",
    "if global_plot_folder_path.exists() and len(list(global_plot_folder_path.glob(\"*\"))) == 0:\n",
    "    shutil.rmtree(global_plot_folder_path)\n",
    "global_plot_folder_path.mkdir(parents=True, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  4.43it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.02it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.35it/s]\n",
      "100%|██████████| 12/12 [00:03<00:00,  3.90it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.24it/s]\n",
      "100%|██████████| 12/12 [00:03<00:00,  3.78it/s]\n",
      "100%|██████████| 6/6 [00:17<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plot the errors from the SAEs, etc...\n",
    "\"\"\"\n",
    "# res folders\n",
    "res_sae_err_norms_output_folder = global_plot_folder_path / \"res_sae_err_norms\" # fmt: skip\n",
    "res_sae_variance_explained_output_folder = global_plot_folder_path / \"res_sae_variance_explained\" # fmt: skip\n",
    "res_sae_mse_output_folder = global_plot_folder_path / \"res_sae_mse\" # fmt: skip\n",
    "\n",
    "# ln2 folders\n",
    "ln2_sae_err_norms_output_folder = global_plot_folder_path / \"ln2_sae_err_norms\" # fmt: skip\n",
    "ln2_sae_variance_explained_output_folder = global_plot_folder_path / \"ln2_sae_variance_explained\" # fmt: skip\n",
    "ln2_sae_mse_output_folder = global_plot_folder_path / \"ln2_sae_mse\" # fmt: skip\n",
    "\n",
    "for (name, folder, arr) in tqdm.tqdm([\n",
    "    # res\n",
    "    (\"res_sae_err_norms\", res_sae_err_norms_output_folder, extracted_activations_flattened.res_sae_error_norms), # fmt: skip\n",
    "    (\"res_sae_variance_explained\", res_sae_variance_explained_output_folder, extracted_activations_flattened.res_sae_var_explained), # fmt: skip\n",
    "    (\"res_sae_mse\", res_sae_mse_output_folder, extracted_activations_flattened.res_sae_mse), # fmt: skip\n",
    "    # ln2\n",
    "    (\"ln2_sae_err_norms\", ln2_sae_err_norms_output_folder, extracted_activations_flattened.ln2_sae_error_norms), # fmt: skip\n",
    "    (\"ln2_sae_variance_explained\", ln2_sae_variance_explained_output_folder, extracted_activations_flattened.ln2_sae_var_explained), # fmt: skip\n",
    "    (\"ln2_sae_mse\", ln2_sae_mse_output_folder, extracted_activations_flattened.ln2_sae_mse), # fmt: skip\n",
    "    \n",
    "]):\n",
    "    if folder.exists() and len(list(folder.glob(\"*\"))) == 0:\n",
    "        shutil.rmtree(folder)\n",
    "    folder.mkdir(parents=True, exist_ok=False)\n",
    "    for layer in tqdm.trange(len(comparer.saes)):\n",
    "        filepath = folder / f\"layer_{layer}.png\"\n",
    "        plt.hist(arr.flatten().cpu().log10().numpy(), bins=100)\n",
    "        plt.title(f\"log10({name}) (layer {layer})\")\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()\n",
    "        filepath_meta = folder / f\"layer_{layer}.json\"\n",
    "        with open(filepath_meta, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"layer\": layer,\n",
    "                \"name\": name,\n",
    "                \"shape\": str(arr.shape),\n",
    "                \"min\": arr.min().item(),\n",
    "                \"max\": arr.max().item(),\n",
    "                \"mean\": arr.mean().item(),\n",
    "                \"std\": arr.std().item(),\n",
    "                \"median\": arr.median().item(),\n",
    "                \"q1\": arr.quantile(0.25).item(),\n",
    "                \"q3\": arr.quantile(0.75).item(),\n",
    "            }, f, indent=4)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing multiple layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:17<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculate the similarity via cosine between each pair of input and output PCs (after\n",
    "applying an SAE). If the SAE is good, we should expect an exact match (i.e. a diagonal-\n",
    "like stripe).\n",
    "\"\"\"\n",
    "plots_output_folder = global_plot_folder_path / \"plots_cosine_sim_pca_post_sae\"\n",
    "if plots_output_folder.exists() and len(list(plots_output_folder.glob(\"*\"))) == 0:\n",
    "    shutil.rmtree(plots_output_folder)\n",
    "plots_output_folder.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "# You can also analyze multiple layers\n",
    "print(\"\\nAnalyzing multiple layers...\")\n",
    "for layer in tqdm.trange(len(comparer.saes)):\n",
    "    # Acquire all the principle components we want to compare\n",
    "    eigenvectors_sae_ins = extracted_activations_flattened.sae_ins_pca_eigenvectors[layer]\n",
    "    eigenvectors_sae_outs = extracted_activations_flattened.sae_outs_pca_eigenvectors[layer]\n",
    "    eigenvectors_ln2s = extracted_activations_flattened.ln2s_pca_eigenvectors[layer]\n",
    "    eigenvectors_ln2s_saed = extracted_activations_flattened.ln2s_saed_pca_eigenvectors[layer]\n",
    "    # Save them to the plot folder\n",
    "    cosine_sim = plot_cosine_kernel(eigenvectors_sae_ins, eigenvectors_sae_outs, force_positive=True, save_to_file=plots_output_folder / f\"layer_{layer}.png\")\n",
    "    cosine_sim = plot_cosine_kernel(eigenvectors_sae_ins, eigenvectors_ln2s, force_positive=True, save_to_file=plots_output_folder / f\"layer_{layer}_sae_ins_ln2.png\")\n",
    "    \n",
    "    # TODO(Adriano) this is some tidbit code written by Claude, not sure if I honestly want it :P\n",
    "    # Find the top aligned eigenvectors\n",
    "    # max_values, max_indices = torch.max(cosine_sim.abs(), dim=1)\n",
    "    # top_k = 5\n",
    "    # top_indices = torch.argsort(max_values, descending=True)[:top_k]\n",
    "    \n",
    "    # print(f\"Top {top_k} aligned eigenvector pairs (SAE in → SAE out):\")\n",
    "    # for i, idx in enumerate(top_indices):\n",
    "    #     out_idx = max_indices[idx]\n",
    "    #     sim_value = cosine_sim[idx, out_idx].item()\n",
    "    #     print(f\"  {i+1}. In eigenvector {idx} aligns with out eigenvector {out_idx} (sim={sim_value:.4f})\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "100%|██████████| 12/12 [00:09<00:00,  1.33it/s]\n",
      " 25%|██▌       | 1/4 [00:09<00:27,  9.03s/it]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|██████████| 12/12 [00:07<00:00,  1.50it/s]\n",
      " 50%|█████     | 2/4 [00:17<00:16,  8.42s/it]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|██████████| 12/12 [00:08<00:00,  1.45it/s]\n",
      " 75%|███████▌  | 3/4 [00:25<00:08,  8.37s/it]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|██████████| 12/12 [00:08<00:00,  1.46it/s]\n",
      "100%|██████████| 4/4 [00:33<00:00,  8.39s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plot for each pair of PCs the histogram of their values on that projection.\n",
    "\"\"\"\n",
    "res_sae_in_pca_histograms_folder = global_plot_folder_path / \"res_sae_in_pca_histograms\"\n",
    "res_sae_out_pca_histograms_folder = global_plot_folder_path / \"res_sae_out_pca_histograms\"\n",
    "ln2_pca_histograms_folder = global_plot_folder_path / \"ln2_pca_histograms\"\n",
    "ln2_sae_effect_pca_histograms_folder = global_plot_folder_path / \"ln2_sae_effect_pca_histograms\"\n",
    "n_pcs = 2 # ehh\n",
    "\n",
    "for output_folder, (activations, mean, eigenvectors) in tqdm.tqdm([\n",
    "    (\n",
    "        res_sae_in_pca_histograms_folder,\n",
    "        (\n",
    "            extracted_activations_flattened.sae_ins,\n",
    "            extracted_activations_flattened.sae_ins_means,\n",
    "            extracted_activations_flattened.sae_ins_pca_eigenvectors\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        res_sae_out_pca_histograms_folder, \n",
    "        (\n",
    "            extracted_activations_flattened.sae_outs,\n",
    "            extracted_activations_flattened.sae_outs_means,\n",
    "            extracted_activations_flattened.sae_outs_pca_eigenvectors\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        ln2_pca_histograms_folder,\n",
    "        (\n",
    "            extracted_activations_flattened.ln2s,\n",
    "            extracted_activations_flattened.ln2s_means,\n",
    "            extracted_activations_flattened.ln2s_pca_eigenvectors\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        ln2_sae_effect_pca_histograms_folder,\n",
    "        (\n",
    "            extracted_activations_flattened.ln2s_saed,\n",
    "            extracted_activations_flattened.ln2s_saed_means,\n",
    "            extracted_activations_flattened.ln2s_saed_pca_eigenvectors\n",
    "        )\n",
    "    )\n",
    "]):\n",
    "    for layer in tqdm.trange(len(comparer.saes)):\n",
    "        output_folder_layer = output_folder / f\"layer_{layer}\"\n",
    "        output_folder_layer.mkdir(parents=True, exist_ok=False)\n",
    "        # NOTE: use default plotting kwargs\n",
    "        plot_all_nc2_top_pcs(n_pcs, activations[layer], mean[layer], eigenvectors[layer], output_folder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m normalize_by_n_in_bin, normalize_by_n_in_bin_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m], [\u001b[33m\"\u001b[39m\u001b[33munnormalized\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnormalized\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m     82\u001b[39m     sub_output_folder = output_folder_layer / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnormalize_by_n_in_bin_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_type_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[43mplot_all_nc2_top_pcs_errs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_pcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Projection parameters\u001b[39;49;00m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43meigenvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Error parameters\u001b[39;49;00m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43merr_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# already layered\u001b[39;49;00m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize_by_n_in_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# already layered\u001b[39;49;00m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Storage parameters\u001b[39;49;00m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43msub_output_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Settings\u001b[39;49;00m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstore_accumulation_values\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize_accumulation_values_by_n_in_bin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize_by_n_in_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# normalize by in bin but not total\u001b[39;49;00m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize_accumulation_values_by_n_total\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnormalize_by_n_in_bin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# expected value if not normalize by in bin; fmt: skip\u001b[39;49;00m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align4_drive2/adrianoh/git2/neel-nanda-mats-2025/LLMDensityestimation/sans_sae_lib/utils.py:242\u001b[39m, in \u001b[36mplot_all_nc2_top_pcs_errs\u001b[39m\u001b[34m(n_pcs, activations, mean, eigenvectors, err_array, normalize_by_n_in_bin, output_folder, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_pcs):\n\u001b[32m    241\u001b[39m    \u001b[38;5;28;01mpass\u001b[39;00m \u001b[38;5;66;03m# XXX here is where we need to finish\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m    \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot implemented\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Not implemented"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plot the error projected onto each pair of PCs as above. Specifically,\n",
    "we accumulate in each bin (same bins as above) the error in the SAE-modified\n",
    "representation in that bin. Then we store 2 versions:\n",
    "1. Divide by the total number of samples (so basically this is proportional to\n",
    "    expected error)\n",
    "2. Divide by the number of samples in that bin (so basically here we are looking to\n",
    "    see if there is an additional contribution of the SAE to that bin above just\n",
    "    \"there were more samples here\")\n",
    "\n",
    "And for each of these two we do it for each of:\n",
    "1. Error norm (on that datapoint)\n",
    "2. Variance explained (on that datapoint)\n",
    "3. MSE (on that datapoint)\n",
    "\"\"\"\n",
    "res_sae_in_pca_histograms_folder = global_plot_folder_path / \"res_sae_in_err_pca_histograms\"\n",
    "res_sae_out_pca_histograms_folder = global_plot_folder_path / \"res_sae_out_err_pca_histograms\"\n",
    "ln2_pca_histograms_folder = global_plot_folder_path / \"ln2_err_pca_histograms\"\n",
    "ln2_sae_effect_pca_histograms_folder = global_plot_folder_path / \"ln2_sae_effect_err_pca_histograms\"\n",
    "n_pcs = 2 # ehh, copy from above lmao\n",
    "for output_folder, (activations, mean, eigenvectors, err_norm, err_var_explained, err_mse) in tqdm.tqdm([\n",
    "    (\n",
    "        res_sae_in_pca_histograms_folder,\n",
    "        (\n",
    "            # Projection (binning) data\n",
    "            extracted_activations_flattened.sae_ins,\n",
    "            extracted_activations_flattened.sae_ins_means,\n",
    "            extracted_activations_flattened.sae_ins_pca_eigenvectors,\n",
    "            # Errors (coloring data)\n",
    "            extracted_activations_flattened.res_sae_error_norms,\n",
    "            extracted_activations_flattened.res_sae_var_explained,\n",
    "            extracted_activations_flattened.res_sae_mse\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        res_sae_out_pca_histograms_folder, \n",
    "        (\n",
    "            # Projection (binning) data\n",
    "            extracted_activations_flattened.sae_outs,\n",
    "            extracted_activations_flattened.sae_outs_means,\n",
    "            extracted_activations_flattened.sae_outs_pca_eigenvectors,\n",
    "            # Errors (coloring data)\n",
    "            extracted_activations_flattened.res_sae_error_norms,\n",
    "            extracted_activations_flattened.res_sae_var_explained,\n",
    "            extracted_activations_flattened.res_sae_mse\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        ln2_pca_histograms_folder,\n",
    "        (\n",
    "            # Projection (binning) data\n",
    "            extracted_activations_flattened.ln2s,\n",
    "            extracted_activations_flattened.ln2s_means,\n",
    "            extracted_activations_flattened.ln2s_pca_eigenvectors,\n",
    "            # Errors (coloring data)\n",
    "            extracted_activations_flattened.ln2_sae_error_norms,\n",
    "            extracted_activations_flattened.ln2_sae_var_explained,\n",
    "            extracted_activations_flattened.ln2_sae_mse\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        ln2_sae_effect_pca_histograms_folder,\n",
    "        (\n",
    "            # Projection (binning) data\n",
    "            extracted_activations_flattened.ln2s_saed,\n",
    "            extracted_activations_flattened.ln2s_saed_means,\n",
    "            extracted_activations_flattened.ln2s_saed_pca_eigenvectors,\n",
    "            # Errors (coloring data)\n",
    "            extracted_activations_flattened.ln2_sae_error_norms,\n",
    "            extracted_activations_flattened.ln2_sae_var_explained,\n",
    "            extracted_activations_flattened.ln2_sae_mse\n",
    "        )\n",
    "    )\n",
    "]):\n",
    "    for layer in tqdm.trange(len(comparer.saes)):\n",
    "        output_folder_layer = output_folder / f\"layer_{layer}\"\n",
    "        output_folder_layer.mkdir(parents=True, exist_ok=False)\n",
    "        err_type_names = [\"error_norm\", \"variance_explained\", \"mse\"] # fmt: skip\n",
    "        err_arrays = [err_norm[layer], err_var_explained[layer], err_mse[layer]] # fmt: skip\n",
    "        for err_type_name, err_array in zip(err_type_names, err_arrays):\n",
    "            for normalize_by_n_in_bin, normalize_by_n_in_bin_name in zip([False, True], [\"unnormalized\", \"normalized\"]):\n",
    "                sub_output_folder = output_folder_layer / f\"{normalize_by_n_in_bin_name}_{err_type_name}\"\n",
    "                plot_all_nc2_top_pcs_errs(\n",
    "                    n_pcs,\n",
    "                    # Projection parameters\n",
    "                    activations[layer],\n",
    "                    mean[layer],\n",
    "                    eigenvectors[layer],\n",
    "                    # Error parameters\n",
    "                    err_array, # already layered\n",
    "                    normalize_by_n_in_bin, # already layered\n",
    "                    # Storage parameters\n",
    "                    sub_output_folder,\n",
    "                    # Settings\n",
    "                    store_accumulation_values=True,\n",
    "                    normalize_accumulation_values_by_n_in_bin=normalize_by_n_in_bin, # normalize by in bin but not total\n",
    "                    normalize_accumulation_values_by_n_total=not normalize_by_n_in_bin # expected value if not normalize by in bin; fmt: skip\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (969763094.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generally the output will look like this:\n",
    "\n",
    "XXX put the filetree and later improve it please!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-density",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
