{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== [Loading Dataset] ==================================================\n",
      "================================================== [Loading Model] ==================================================\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\"\"\"\n",
    "TODO(Adriano) after getting some plots for the blobs post-SAE in GPT2, it's important to check whether these\n",
    "SAEs are actually any good. Unfortunately, I have really bad FVUs, MSEs, etc... It's also unclear if a error\n",
    "norm of 30 is normal. People seem not to be reporting this very well and it's deeply annoying.\n",
    "\n",
    "TODO(Adriano) we want to get rid of EOS\n",
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import dotenv\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load our own imports, etc...\n",
    "from sans_sae_lib.utils import plot_cosine_kernel, plot_all_nc2_top_pcs, plot_all_nc2_top_pcs_errs\n",
    "from sans_sae_lib.schemas import ExtractedActivations, FlattenedExtractedActivations\n",
    "from sans_sae_main import ResidAndLn2Comparer\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "assert \"CUDA_VISIBLE_DEVICES\" in os.environ, \"CUDA_VISIBLE_DEVICES is not set\"\n",
    "assert len(os.environ[\"CUDA_VISIBLE_DEVICES\"].strip()) > 0, \"CUDA_VISIBLE_DEVICES is empty\"\n",
    "\n",
    "print(\"=\"*50 + \" [Loading Dataset] \" + \"=\"*50) # DEBUG\n",
    "# dataset = load_dataset(\"openwebtext\", split=\"train\", trust_remote_code=True)\n",
    "dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\", trust_remote_code=True) # Smaller version\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    # NOTE: all these have context 128\n",
    "    max_length=128, #sae.cfg.context_size,\n",
    "    add_bos_token=True, #sae.cfg.prepend_bos,\n",
    ")\n",
    "print(\"=\"*50 + \" [Loading Model] \" + \"=\"*50) # DEBUG\n",
    "# TODO(Adriano) do this below...\n",
    "# for d in extractor.cfg_dics:\n",
    "#     print(d[\"context_size\"]) # NOTE: you should picke the smallest of these...\n",
    "\n",
    "\n",
    "# Shorten the dataset for testing more quickly\n",
    "dataset_size = 300 # XXX make this longer please\n",
    "token_dataset_short = token_dataset[:dataset_size]['tokens']\n",
    "dataset_length = token_dataset_short.shape[0]\n",
    "sequence_length = token_dataset_short.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== [Loading Model] ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/sae_lens/sae.py:151: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50 + \" [Loading Model] \" + \"=\"*50) # DEBUG\n",
    "comparer = ResidAndLn2Comparer()\n",
    "# print(comparer.model) # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== [Extracting Activations] ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 150:   0%|          | 0/2 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 42.19 GiB. GPU 0 has a total capacity of 79.11 GiB of which 21.96 GiB is free. Including non-PyTorch memory, this process has 57.13 GiB memory in use. Of the allocated memory 54.72 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 135:   0%|          | 0/3 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 37.97 GiB. GPU 0 has a total capacity of 79.11 GiB of which 11.85 GiB is free. Including non-PyTorch memory, this process has 67.24 GiB memory in use. Of the allocated memory 49.48 GiB is allocated by PyTorch, and 17.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 121:   0%|          | 0/3 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 34.03 GiB. GPU 0 has a total capacity of 79.11 GiB of which 18.13 GiB is free. Including non-PyTorch memory, this process has 60.96 GiB memory in use. Of the allocated memory 44.59 GiB is allocated by PyTorch, and 15.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 108:   0%|          | 0/3 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 30.38 GiB. GPU 0 has a total capacity of 79.11 GiB of which 23.36 GiB is free. Including non-PyTorch memory, this process has 55.73 GiB memory in use. Of the allocated memory 40.05 GiB is allocated by PyTorch, and 15.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 97:   0%|          | 0/4 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 27.28 GiB. GPU 0 has a total capacity of 79.11 GiB of which 448.88 MiB is free. Including non-PyTorch memory, this process has 78.65 GiB memory in use. Of the allocated memory 63.49 GiB is allocated by PyTorch, and 14.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 87:   0%|          | 0/4 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 24.47 GiB. GPU 0 has a total capacity of 79.11 GiB of which 9.44 GiB is free. Including non-PyTorch memory, this process has 69.65 GiB memory in use. Of the allocated memory 57.19 GiB is allocated by PyTorch, and 11.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 78:   0%|          | 0/4 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 21.94 GiB. GPU 0 has a total capacity of 79.11 GiB of which 16.75 GiB is free. Including non-PyTorch memory, this process has 62.34 GiB memory in use. Of the allocated memory 51.51 GiB is allocated by PyTorch, and 10.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 70:   0%|          | 0/5 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 9.84 GiB. GPU 0 has a total capacity of 79.11 GiB of which 3.39 GiB is free. Including non-PyTorch memory, this process has 75.70 GiB memory in use. Of the allocated memory 66.16 GiB is allocated by PyTorch, and 8.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 63:   0%|          | 0/5 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.11 GiB of which 448.88 MiB is free. Including non-PyTorch memory, this process has 78.65 GiB memory in use. Of the allocated memory 68.64 GiB is allocated by PyTorch, and 9.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 56:   0%|          | 0/6 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 15.75 GiB. GPU 0 has a total capacity of 79.11 GiB of which 15.20 GiB is free. Including non-PyTorch memory, this process has 63.89 GiB memory in use. Of the allocated memory 61.27 GiB is allocated by PyTorch, and 2.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 50:   0%|          | 0/6 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 21.10 GiB. GPU 0 has a total capacity of 79.11 GiB of which 3.18 GiB is free. Including non-PyTorch memory, this process has 75.91 GiB memory in use. Of the allocated memory 69.02 GiB is allocated by PyTorch, and 6.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 45:   0%|          | 0/7 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 18.99 GiB. GPU 0 has a total capacity of 79.11 GiB of which 10.21 GiB is free. Including non-PyTorch memory, this process has 68.88 GiB memory in use. Of the allocated memory 62.35 GiB is allocated by PyTorch, and 5.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 40:  12%|█▎        | 1/8 [00:07<00:49,  7.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA out of memory. Tried to allocate 16.88 GiB. GPU 0 has a total capacity of 79.11 GiB of which 11.97 GiB is free. Including non-PyTorch memory, this process has 67.12 GiB memory in use. Of the allocated memory 58.85 GiB is allocated by PyTorch, and 7.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "REDUCING BATCH SIZE FOR NEXT ITERATION TO 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Size = 36: 100%|██████████| 9/9 [00:41<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln2s.shape=torch.Size([12, 300, 128, 768])\n",
      "sae_outs_per_k.shape=torch.Size([9, 12, 300, 128, 768])\n",
      "sae_ins.shape=torch.Size([12, 300, 128, 768])\n",
      "ln2s_saed_per_k.shape=torch.Size([9, 12, 300, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50 + \" [Extracting Activations] \" + \"=\"*50) # DEBUG\n",
    "extracted_activations: ExtractedActivations = comparer.extract_activations(\n",
    "    token_dataset_short,\n",
    "    batch_size=40 # Turns out to work on my machine :)\n",
    ")\n",
    "print(f\"ln2s.shape={extracted_activations.ln2s.shape}\")\n",
    "print(f\"sae_outs_per_k.shape={extracted_activations.sae_outs_per_k.shape}\")\n",
    "print(f\"sae_ins.shape={extracted_activations.sae_ins.shape}\")\n",
    "print(f\"ln2s_saed_per_k.shape={extracted_activations.ln2s_saed_per_k.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== [Tesing that encoding and then decoding = just forwards] ==================================================\n",
      "================================================== [OK... enough?] ==================================================\n",
      "================================================== [Tesing that apply sae equally good to forward] ==================================================\n",
      "================================================== [OK... enough?] ==================================================\n",
      "================================================== [Tesing that TopK works as intended] ==================================================\n",
      "================================================== [OK... enough?] ==================================================\n",
      "================================================== [Tesing that TopK forcing is not clearly wrong] ==================================================\n",
      "================================================== [OK (enough??)] ==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DEBUG BLOCK. This block was used to make sure that the top-k forcing is not buggy.\n",
    "\"\"\"\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    print(\"=\"*50 + \" [Tesing that encoding and then decoding = just forwards] \" + \"=\"*50) # DEBUG\n",
    "    def test_encode_decode_consistency():\n",
    "        # Test that encoding and decoding are consistent\n",
    "        # Create a random tensor of shape (batch, seq, d_model)\n",
    "        n_layers = 12\n",
    "        batch_size = 10\n",
    "        seq_length = 128\n",
    "        d_model = 768\n",
    "        test_tensor = torch.randn(n_layers, batch_size, seq_length, d_model).to(comparer.device) * 20\n",
    "        encoded = torch.stack([sae.encode(test_tensor) for sae in comparer.saes])\n",
    "        decoded = torch.stack([sae.decode(encoded[i]) for i, sae in enumerate(comparer.saes)])\n",
    "        passed_forwards = torch.stack([sae(test_tensor) for sae in comparer.saes])\n",
    "        mae = (passed_forwards - decoded).abs().mean()\n",
    "        assert torch.allclose(passed_forwards, decoded), f\"MAE: {mae}\"\n",
    "    test_encode_decode_consistency()\n",
    "    print(\"=\"*50 + \" [OK... enough?] \" + \"=\"*50) # DEBUG\n",
    "\n",
    "    print(\"=\"*50 + \" [Tesing that apply sae equally good to forward] \" + \"=\"*50) # DEBUG\n",
    "    def test_apply_sae_equally_good_to_forward():\n",
    "        # Test that encoding and decoding are consistent\n",
    "        # Create a random tensor of shape (batch, seq, d_model)\n",
    "        n_layers = 12\n",
    "        batch_size = 10\n",
    "        seq_length = 128\n",
    "        d_model = 768\n",
    "        test_tensor = torch.randn(n_layers, batch_size, seq_length, d_model).to(comparer.device) * 20\n",
    "        passed_apply = comparer.apply_sae(test_tensor)\n",
    "        passed_forwards = torch.stack([sae(test_tensor[i]) for i, sae in enumerate(comparer.saes)])\n",
    "        mae = (passed_apply - passed_forwards).abs().mean()\n",
    "        assert torch.allclose(passed_apply, passed_forwards), f\"MAE: {mae}\"\n",
    "    test_apply_sae_equally_good_to_forward()\n",
    "    print(\"=\"*50 + \" [OK... enough?] \" + \"=\"*50) # DEBUG\n",
    "\n",
    "    print(\"=\"*50 + \" [Tesing that TopK works as intended] \" + \"=\"*50) # DEBUG\n",
    "    def test_topk_works_as_intended():\n",
    "        # Test that TopK works as intended\n",
    "        # Create a random tensor of shape (batch, seq, d_model)\n",
    "        n_layers = 12\n",
    "        batch_size = 10\n",
    "        seq_length = 128\n",
    "        d_model = 768\n",
    "        test_tensor = torch.randn(n_layers, batch_size, seq_length, d_model).to(comparer.device) * 20\n",
    "        test_tensor = F.relu(test_tensor)\n",
    "        assert test_tensor.min().item() >= 0, f\"test_tensor.min()={test_tensor.min().item()}\"\n",
    "        k = d_model\n",
    "        topk_values, topk_indices = torch.topk(test_tensor, k=k, dim=-1)\n",
    "        mask = torch.zeros_like(test_tensor)\n",
    "        mask.scatter_(-1, topk_indices, topk_values)\n",
    "        assert torch.allclose(test_tensor, mask), f\"test_tensor.min()={test_tensor.min().item()}\"\n",
    "    test_topk_works_as_intended()\n",
    "    print(\"=\"*50 + \" [OK... enough?] \" + \"=\"*50) # DEBUG\n",
    "\n",
    "    print(\"=\"*50 + \" [Tesing that TopK forcing is not clearly wrong] \" + \"=\"*50) # DEBUG\n",
    "    def test_forcing_ok():\n",
    "        # Test that forcing topk is not clearly wrong by making sure that\n",
    "        # if k >= 768 you get the same result as just applying the SAE normally\n",
    "        # Create a random tensor of shape (batch, seq, d_model)\n",
    "        n_layers = 12\n",
    "        batch_size = 10\n",
    "        seq_length = 128\n",
    "        d_model = 768\n",
    "        test_tensor = torch.randn(n_layers, batch_size, seq_length, d_model).to(comparer.device) * 20\n",
    "        d_sae = comparer.saes[0].encode(test_tensor[0]).shape[-1]\n",
    "        unforced = comparer.apply_sae(test_tensor)\n",
    "        forced_similar = comparer.apply_sae(test_tensor, force_topk=d_sae)\n",
    "        forced_dissimilar = comparer.apply_sae(test_tensor, force_topk=1)\n",
    "        mae_similar = (unforced - forced_similar).abs().mean()\n",
    "        mae_dissimilar = (unforced - forced_dissimilar).abs().mean()\n",
    "        assert not torch.allclose(unforced, forced_dissimilar), f\"MAE: {mae_dissimilar}\" # run this first since usually passes\n",
    "        assert torch.allclose(unforced, forced_similar), f\"MAE: {mae_similar} (dissimilar: {mae_dissimilar})\"\n",
    "        \n",
    "    test_forcing_ok()\n",
    "    print(\"=\"*50 + \" [OK (enough??)] \" + \"=\"*50) # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== [Flattening + Calculating PCA & Errors] ==================================================\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Not implemented need to support multiple ks",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m [Flattening + Calculating PCA & Errors] \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m) \u001b[38;5;66;03m# DEBUG\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m extracted_activations_flattened: FlattenedExtractedActivations = \u001b[43mextracted_activations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m [Done, now ready to plot (next cell)] \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m) \u001b[38;5;66;03m# DEBUG\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align4_drive2/adrianoh/git2/neel-nanda-mats-2025/LLMDensityestimation/sans_sae_lib/schemas.py:164\u001b[39m, in \u001b[36mExtractedActivations.flatten\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> FlattenedExtractedActivations:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# 1. Reshape to ignore batch dimension\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot implemented need to support multiple ks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    165\u001b[39m     flatten_pattern = \u001b[33m\"\u001b[39m\u001b[33mlayer batch seq d_model -> layer (batch seq) d_model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m     flatten_func = \u001b[38;5;28;01mlambda\u001b[39;00m x: einops.rearrange(x, flatten_pattern)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Not implemented need to support multiple ks"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50 + \" [Flattening + Calculating PCA & Errors] \" + \"=\"*50) # DEBUG\n",
    "extracted_activations_flattened: FlattenedExtractedActivations = extracted_activations.flatten()\n",
    "print(\"=\"*50 + \" [Done, now ready to plot (next cell)] \" + \"=\"*50) # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the global plots folder for us to be able to plot everything\"\"\"\n",
    "global_plot_folder_path = Path(\"sae_sans_plots\")\n",
    "if global_plot_folder_path.exists() and len(list(global_plot_folder_path.glob(\"*\"))) == 0:\n",
    "    shutil.rmtree(global_plot_folder_path)\n",
    "global_plot_folder_path.mkdir(parents=True, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  4.11it/s]\n",
      "100%|██████████| 12/12 [00:03<00:00,  3.89it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.32it/s]\n",
      "100%|██████████| 12/12 [00:03<00:00,  3.79it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.47it/s]\n",
      "100%|██████████| 12/12 [00:03<00:00,  3.85it/s]\n",
      "100%|██████████| 6/6 [00:17<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plot the errors from the SAEs, etc...\n",
    "\"\"\"\n",
    "# res folders\n",
    "res_sae_err_norms_output_folder = global_plot_folder_path / \"res_sae_err_norms\" # fmt: skip\n",
    "res_sae_variance_explained_output_folder = global_plot_folder_path / \"res_sae_variance_explained\" # fmt: skip\n",
    "res_sae_mse_output_folder = global_plot_folder_path / \"res_sae_mse\" # fmt: skip\n",
    "\n",
    "# ln2 folders\n",
    "ln2_sae_err_norms_output_folder = global_plot_folder_path / \"ln2_sae_err_norms\" # fmt: skip\n",
    "ln2_sae_variance_explained_output_folder = global_plot_folder_path / \"ln2_sae_variance_explained\" # fmt: skip\n",
    "ln2_sae_mse_output_folder = global_plot_folder_path / \"ln2_sae_mse\" # fmt: skip\n",
    "\n",
    "for (name, folder, arr_per_k) in tqdm.tqdm([\n",
    "    # res\n",
    "    (\"res_sae_err_norms\", res_sae_err_norms_output_folder, extracted_activations_flattened.res_sae_error_norms), # fmt: skip\n",
    "    (\"res_sae_variance_explained\", res_sae_variance_explained_output_folder, extracted_activations_flattened.res_sae_var_explained), # fmt: skip\n",
    "    (\"res_sae_mse\", res_sae_mse_output_folder, extracted_activations_flattened.res_sae_mse), # fmt: skip\n",
    "    # ln2\n",
    "    (\"ln2_sae_err_norms\", ln2_sae_err_norms_output_folder, extracted_activations_flattened.ln2_sae_error_norms), # fmt: skip\n",
    "    (\"ln2_sae_variance_explained\", ln2_sae_variance_explained_output_folder, extracted_activations_flattened.ln2_sae_var_explained), # fmt: skip\n",
    "    (\"ln2_sae_mse\", ln2_sae_mse_output_folder, extracted_activations_flattened.ln2_sae_mse), # fmt: skip\n",
    "    \n",
    "]):\n",
    "    if folder.exists() and len(list(folder.glob(\"*\"))) == 0:\n",
    "        shutil.rmtree(folder)\n",
    "    folder.mkdir(parents=True, exist_ok=False)\n",
    "    # Check that the first two dimensions are k and layer\n",
    "    assert arr_per_k.shape[0] == len(comparer.forced_ks), f\"arr_per_k.shape[0]={arr_per_k.shape[0]}, need {len(comparer.forced_ks)}\" # fmt: skip\n",
    "    assert arr_per_k.shape[1] == len(comparer.saes), f\"arr_per_k.shape[1]={arr_per_k.shape[1]}, need {len(comparer.saes)}\" # fmt: skip\n",
    "    for layer in tqdm.trange(len(comparer.saes)):\n",
    "        for kidx, k in tqdm.tqdm(enumerate(comparer.forced_ks), total=len(comparer.forced_ks), desc=f\"layer {layer} x ks\"):\n",
    "            arr = arr_per_k[kidx, layer]\n",
    "            assert arr.ndim == 1 # just a bunch of errors, etc...\n",
    "            filepath = folder / f\"layer_{layer}_k_{k}.png\"\n",
    "            plt.hist(arr.cpu().log10().numpy(), bins=100)\n",
    "            plt.title(f\"log10({name}) (layer {layer}) @ top=k {k}\")\n",
    "            plt.savefig(filepath)\n",
    "            plt.close()\n",
    "            filepath_meta = folder / f\"layer_{layer}_k_{k}.json\"\n",
    "            with open(filepath_meta, \"w\") as f:\n",
    "                json.dump({\n",
    "                    \"layer\": layer,\n",
    "                    \"name\": name,\n",
    "                    \"shape\": str(arr.shape),\n",
    "                    \"min\": arr.min().item(),\n",
    "                    \"max\": arr.max().item(),\n",
    "                    \"mean\": arr.mean().item(),\n",
    "                    \"std\": arr.std().item(),\n",
    "                    \"median\": arr.median().item(),\n",
    "                    \"q1\": arr.quantile(0.25).item(),\n",
    "                    \"q3\": arr.quantile(0.75).item(),\n",
    "                    \"k\": k\n",
    "                }, f, indent=4)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing multiple layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:19<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculate the similarity via cosine between each pair of input and output PCs (after\n",
    "applying an SAE). If the SAE is good, we should expect an exact match (i.e. a diagonal-\n",
    "like stripe).\n",
    "\"\"\"\n",
    "plots_output_folder = global_plot_folder_path / \"plots_cosine_sim_pca_post_sae\"\n",
    "if plots_output_folder.exists() and len(list(plots_output_folder.glob(\"*\"))) == 0:\n",
    "    shutil.rmtree(plots_output_folder)\n",
    "plots_output_folder.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "# You can also analyze multiple layers\n",
    "print(\"\\nAnalyzing multiple layers...\")\n",
    "for layer in tqdm.trange(len(comparer.saes)):\n",
    "    # Acquire all the principle components we want to compare\n",
    "    eigenvectors_sae_ins = extracted_activations_flattened.sae_ins_pca_eigenvectors[layer] # fmt: skip\n",
    "    eigenvectors_ln2s = extracted_activations_flattened.ln2s_pca_eigenvectors[layer] # fmt: skip\n",
    "    for kidx, k in tqdm.tqdm(enumerate(comparer.forced_ks), total=len(comparer.forced_ks), desc=f\"layer {layer} x ks\"): # fmt: skip\n",
    "        eigenvectors_sae_outs = extracted_activations_flattened.sae_outs_per_k_pca_eigenvectors[kidx, layer] # fmt: skip\n",
    "        eigenvectors_ln2s_saed = extracted_activations_flattened.ln2s_saed_per_k_pca_eigenvectors[kidx, layer] # fmt: skip\n",
    "        assert eigenvectors_sae_outs.shape == eigenvectors_sae_ins.shape, f\"eigenvectors_sae_outs.shape={eigenvectors_sae_outs.shape}, eigenvectors_sae_ins.shape={eigenvectors_sae_ins.shape}\" # fmt: skip\n",
    "        assert eigenvectors_ln2s_saed.shape == eigenvectors_ln2s.shape, f\"eigenvectors_ln2s_saed.shape={eigenvectors_ln2s_saed.shape}, eigenvectors_ln2s.shape={eigenvectors_ln2s.shape}\" # fmt: skip\n",
    "        # Save them to the plot folder\n",
    "        cosine_sim = plot_cosine_kernel(eigenvectors_sae_ins, eigenvectors_sae_outs, force_positive=True, save_to_file=plots_output_folder / f\"layer_{layer}_k_{k}_res.png\") # fmt: skip\n",
    "        cosine_sim = plot_cosine_kernel(eigenvectors_ln2s, eigenvectors_ln2s, force_positive=True, save_to_file=plots_output_folder / f\"layer_{layer}_k_{k}_ln2.png\") # fmt: skip\n",
    "    \n",
    "    # TODO(Adriano) this is some tidbit code written by Claude, not sure if I honestly want it :P\n",
    "    # Find the top aligned eigenvectors\n",
    "    # max_values, max_indices = torch.max(cosine_sim.abs(), dim=1)\n",
    "    # top_k = 5\n",
    "    # top_indices = torch.argsort(max_values, descending=True)[:top_k]\n",
    "    \n",
    "    # print(f\"Top {top_k} aligned eigenvector pairs (SAE in → SAE out):\")\n",
    "    # for i, idx in enumerate(top_indices):\n",
    "    #     out_idx = max_indices[idx]\n",
    "    #     sim_value = cosine_sim[idx, out_idx].item()\n",
    "    #     print(f\"  {i+1}. In eigenvector {idx} aligns with out eigenvector {out_idx} (sim={sim_value:.4f})\")\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 12/12 [00:09<00:00,  1.32it/s]\n",
      " 25%|██▌       | 1/4 [00:09<00:27,  9.08s/it]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "100%|██████████| 12/12 [00:07<00:00,  1.57it/s]\n",
      " 50%|█████     | 2/4 [00:16<00:16,  8.23s/it]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "100%|██████████| 12/12 [00:08<00:00,  1.45it/s]\n",
      " 75%|███████▌  | 3/4 [00:25<00:08,  8.26s/it]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|██████████| 12/12 [00:08<00:00,  1.49it/s]\n",
      "100%|██████████| 4/4 [00:33<00:00,  8.27s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plot for each pair of PCs the histogram of their values on that projection.\n",
    "\"\"\"\n",
    "res_sae_in_pca_histograms_folder = global_plot_folder_path / \"res_sae_in_pca_histograms\"\n",
    "res_sae_out_pca_histograms_folder = global_plot_folder_path / \"res_sae_out_pca_histograms\"\n",
    "ln2_pca_histograms_folder = global_plot_folder_path / \"ln2_pca_histograms\"\n",
    "ln2_sae_effect_pca_histograms_folder = global_plot_folder_path / \"ln2_sae_effect_pca_histograms\"\n",
    "n_pcs = 2 # ehh\n",
    "\n",
    "for output_folder, (activations, mean, eigenvectors), per_k in tqdm.tqdm([\n",
    "    (\n",
    "        res_sae_in_pca_histograms_folder,\n",
    "        (\n",
    "            extracted_activations_flattened.sae_ins,\n",
    "            extracted_activations_flattened.sae_ins_means,\n",
    "            extracted_activations_flattened.sae_ins_pca_eigenvectors\n",
    "        ),\n",
    "        False\n",
    "    ),\n",
    "    (\n",
    "        res_sae_out_pca_histograms_folder, \n",
    "        (\n",
    "            extracted_activations_flattened.sae_outs_per_k,\n",
    "            extracted_activations_flattened.sae_outs_per_k_means,\n",
    "            extracted_activations_flattened.sae_outs_per_k_pca_eigenvectors\n",
    "        ),\n",
    "        True\n",
    "    ),\n",
    "    (\n",
    "        ln2_pca_histograms_folder,\n",
    "        (\n",
    "            extracted_activations_flattened.ln2s,\n",
    "            extracted_activations_flattened.ln2s_means,\n",
    "            extracted_activations_flattened.ln2s_pca_eigenvectors\n",
    "        ),\n",
    "        False\n",
    "    ),\n",
    "    (\n",
    "        ln2_sae_effect_pca_histograms_folder,\n",
    "        (\n",
    "            extracted_activations_flattened.ln2s_saed_per_k,\n",
    "            extracted_activations_flattened.ln2s_saed_per_k_means,\n",
    "            extracted_activations_flattened.ln2s_saed_per_k_pca_eigenvectors\n",
    "        ),\n",
    "        True\n",
    "    )\n",
    "]):\n",
    "    for layer in tqdm.trange(len(comparer.saes)):\n",
    "        output_folder_layer = output_folder / (f\"layer_{layer}\" + (f\"_k_{k}\" if per_k else \"\"))\n",
    "        output_folder_layer.mkdir(parents=True, exist_ok=False)\n",
    "        if per_k:\n",
    "            for kidx, k in tqdm.tqdm(enumerate(comparer.forced_ks), total=len(comparer.forced_ks), desc=f\"layer {layer} x ks\"): # fmt: skip\n",
    "                plot_all_nc2_top_pcs(n_pcs, activations[kidx, layer], mean[kidx, layer], eigenvectors[kidx, layer], output_folder_layer)\n",
    "        else:\n",
    "            plot_all_nc2_top_pcs(n_pcs, activations[layer], mean[layer], eigenvectors[layer], output_folder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "\n",
      "\u001b[A/mnt/align4_drive2/adrianoh/git2/neel-nanda-mats-2025/LLMDensityestimation/sans_sae_lib/utils.py:192: RuntimeWarning: invalid value encountered in log1p\n",
      "  plt.imshow(np.log1p(H).T, origin=\"lower\", aspect=\"auto\",\n",
      "\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 12/12 [00:46<00:00,  3.84s/it]\n",
      " 25%|██▌       | 1/4 [00:46<02:18, 46.10s/it]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "100%|██████████| 12/12 [00:50<00:00,  4.17s/it]\n",
      " 50%|█████     | 2/4 [01:36<01:36, 48.42s/it]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "100%|██████████| 12/12 [00:51<00:00,  4.32s/it]\n",
      " 75%|███████▌  | 3/4 [02:27<00:49, 49.99s/it]\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Plotting PCA histograms: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "100%|██████████| 12/12 [00:52<00:00,  4.35s/it]\n",
      "100%|██████████| 4/4 [03:20<00:00, 50.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "# import importlib\n",
    "# import sans_sae_lib.utils\n",
    "# importlib.reload(sans_sae_lib.utils)\n",
    "# plot_all_nc2_top_pcs_errs = sans_sae_lib.utils.plot_all_nc2_top_pcs_errs\n",
    "\"\"\"\n",
    "Plot the error projected onto each pair of PCs as above. Specifically,\n",
    "we accumulate in each bin (same bins as above) the error in the SAE-modified\n",
    "representation in that bin. Then we store 2 versions:\n",
    "1. Divide by the total number of samples (so basically this is proportional to\n",
    "    expected error)\n",
    "2. Divide by the number of samples in that bin (so basically here we are looking to\n",
    "    see if there is an additional contribution of the SAE to that bin above just\n",
    "    \"there were more samples here\")\n",
    "\n",
    "And for each of these two we do it for each of:\n",
    "1. Error norm (on that datapoint)\n",
    "2. Variance explained (on that datapoint)\n",
    "3. MSE (on that datapoint)\n",
    "\"\"\"\n",
    "res_sae_in_pca_histograms_folder = global_plot_folder_path / \"res_sae_in_err_pca_histograms\"\n",
    "res_sae_out_pca_histograms_folder = global_plot_folder_path / \"res_sae_out_err_pca_histograms\"\n",
    "ln2_pca_histograms_folder = global_plot_folder_path / \"ln2_err_pca_histograms\"\n",
    "ln2_sae_effect_pca_histograms_folder = global_plot_folder_path / \"ln2_sae_effect_err_pca_histograms\"\n",
    "n_pcs = 2 # ehh, copy from above lmao\n",
    "for output_folder, (activations, mean, eigenvectors, err_norm, err_var_explained, err_mse) in tqdm.tqdm([\n",
    "    (\n",
    "        res_sae_in_pca_histograms_folder,\n",
    "        (\n",
    "            # Projection (binning) data\n",
    "            extracted_activations_flattened.sae_ins,\n",
    "            extracted_activations_flattened.sae_ins_means,\n",
    "            extracted_activations_flattened.sae_ins_pca_eigenvectors,\n",
    "            # Errors (coloring data)\n",
    "            extracted_activations_flattened.res_sae_error_norms,\n",
    "            extracted_activations_flattened.res_sae_var_explained,\n",
    "            extracted_activations_flattened.res_sae_mse\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        res_sae_out_pca_histograms_folder, \n",
    "        (\n",
    "            # Projection (binning) data\n",
    "            extracted_activations_flattened.sae_outs,\n",
    "            extracted_activations_flattened.sae_outs_means,\n",
    "            extracted_activations_flattened.sae_outs_pca_eigenvectors,\n",
    "            # Errors (coloring data)\n",
    "            extracted_activations_flattened.res_sae_error_norms,\n",
    "            extracted_activations_flattened.res_sae_var_explained,\n",
    "            extracted_activations_flattened.res_sae_mse\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        ln2_pca_histograms_folder,\n",
    "        (\n",
    "            # Projection (binning) data\n",
    "            extracted_activations_flattened.ln2s,\n",
    "            extracted_activations_flattened.ln2s_means,\n",
    "            extracted_activations_flattened.ln2s_pca_eigenvectors,\n",
    "            # Errors (coloring data)\n",
    "            extracted_activations_flattened.ln2_sae_error_norms,\n",
    "            extracted_activations_flattened.ln2_sae_var_explained,\n",
    "            extracted_activations_flattened.ln2_sae_mse\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        ln2_sae_effect_pca_histograms_folder,\n",
    "        (\n",
    "            # Projection (binning) data\n",
    "            extracted_activations_flattened.ln2s_saed,\n",
    "            extracted_activations_flattened.ln2s_saed_means,\n",
    "            extracted_activations_flattened.ln2s_saed_pca_eigenvectors,\n",
    "            # Errors (coloring data)\n",
    "            extracted_activations_flattened.ln2_sae_error_norms,\n",
    "            extracted_activations_flattened.ln2_sae_var_explained,\n",
    "            extracted_activations_flattened.ln2_sae_mse\n",
    "        )\n",
    "    )\n",
    "]):\n",
    "    for layer in tqdm.trange(len(comparer.saes)):\n",
    "        output_folder_layer = output_folder / f\"layer_{layer}\"\n",
    "        output_folder_layer.mkdir(parents=True, exist_ok=False)\n",
    "        err_type_names = [\"error_norm\", \"variance_explained\", \"mse\"] # fmt: skip\n",
    "        err_arrays = [err_norm[layer], err_var_explained[layer], err_mse[layer]] # fmt: skip\n",
    "        for err_type_name, err_array in zip(err_type_names, err_arrays):\n",
    "            for normalize_by_n_in_bin, normalize_by_n_in_bin_name in zip([False, True], [\"unnormalized\", \"normalized\"]):\n",
    "                sub_output_folder = output_folder_layer / f\"{normalize_by_n_in_bin_name}_{err_type_name}\"\n",
    "                plot_all_nc2_top_pcs_errs(\n",
    "                    n_pcs,\n",
    "                    # Projection parameters\n",
    "                    activations[layer],\n",
    "                    mean[layer],\n",
    "                    eigenvectors[layer],\n",
    "                    # Error parameters + Plotting n stuff\n",
    "                    err_array, # already layered\n",
    "                    normalize_by_n_in_bin, # normalize by in bin but not total\n",
    "                    not normalize_by_n_in_bin, # normalize by total not in bin\n",
    "                    # Storage parameters\n",
    "                    sub_output_folder,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGenerally the output will look like this:\\n\\nsae_sans_plots\\n├── ln2_err_pca_histograms\\n│   ├── layer_0\\n│   │   ├── normalized_error_norm\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── normalized_mse\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── normalized_variance_explained\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── unnormalized_error_norm\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── unnormalized_mse\\n│   │   │   └── pc1_pc2.png\\n│   │   └── unnormalized_variance_explained\\n│   │       └── pc1_pc2.png\\n│   ...\\n├── ln2_pca_histograms\\n│   ├── layer_0\\n│   │   └── pc1_pc2.png\\n│   ...\\n├── ln2_sae_effect_err_pca_histograms\\n│   ├── layer_0\\n│   │   ├── normalized_error_norm\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── normalized_mse\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── normalized_variance_explained\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── unnormalized_error_norm\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── unnormalized_mse\\n│   │   │   └── pc1_pc2.png\\n│   │   └── unnormalized_variance_explained\\n│   │       └── pc1_pc2.png\\n│   ...\\n├── ln2_sae_effect_pca_histograms\\n│   ├── layer_0\\n│   │   └── pc1_pc2.png\\n│   ├── layer_1\\n│   │   └── pc1_pc2.png\\n│   ...\\n├── ln2_sae_err_norms\\n│   ├── layer_0.json\\n│   ├── layer_0.png\\n│   ...\\n├── ln2_sae_mse\\n│   ├── layer_0.json\\n│   ├── layer_0.png\\n│   ...\\n├── ln2_sae_variance_explained\\n│   ├── layer_0.json\\n│   ├── layer_0.png\\n│   ...\\n├── plots_cosine_sim_pca_post_sae\\n│   ├── layer_0.png\\n│   ├── layer_0_sae_ins_ln2.png\\n│   ...\\n├── res_sae_err_norms\\n│   ├── layer_0.json\\n│   ├── layer_0.png\\n│   ...\\n├── res_sae_in_err_pca_histograms\\n│   ├── layer_0\\n│   │   ├── normalized_error_norm\\n│   │   │   └── pc1_pc2.png\\n|   |   |   ... (there might be more pairs of PCs)\\n│   │   ├── normalized_mse\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── normalized_variance_explained\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── unnormalized_error_norm\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── unnormalized_mse\\n│   │   │   └── pc1_pc2.png\\n│   │   └── unnormalized_variance_explained\\n│   │       └── pc1_pc2.png\\n│   ...\\n├── res_sae_in_pca_histograms\\n│   ├── layer_0\\n│   │   └── pc1_pc2.png\\n│   ├── layer_1\\n│   │   └── pc1_pc2.png\\n│   ...\\n├── res_sae_mse\\n│   ├── layer_0.json\\n│   ├── layer_0.png\\n│   ├── ...\\n|   ...\\n├── res_sae_out_err_pca_histograms\\n│   ├── layer_0\\n│   │   ├── normalized_error_norm\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── normalized_mse\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── normalized_variance_explained\\n│   │   │   └── pc1_pc2.png\\n│   │   ├── unnormalized_error_norm\\n│   │   │   └── pc1_pc2.png\\n|   |   |   ... (there might be more pairs of PCs)\\n│   │   ├── unnormalized_mse\\n│   │   │   └── pc1_pc2.png\\n│   │   └── unnormalized_variance_explained\\n│   │       └── pc1_pc2.png\\n│   ├── layer_1\\n│   │   ├── normalized_error_norm\\n│   │   ...\\n|   ...\\n├── res_sae_out_pca_histograms\\n│   ├── layer_0\\n│   │   └── pc1_pc2.png\\n│   ├── layer_1\\n│   │   └── pc1_pc2.png\\n│   ...\\n└── res_sae_variance_explained\\n    ├── layer_0.json\\n    ├── layer_0.png\\n    ├── ...\\n    ...\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generally the output will look like this:\n",
    "\n",
    "sae_sans_plots\n",
    "├── ln2_err_pca_histograms\n",
    "│   ├── layer_0\n",
    "│   │   ├── normalized_error_norm\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── normalized_mse\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── normalized_variance_explained\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── unnormalized_error_norm\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── unnormalized_mse\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   └── unnormalized_variance_explained\n",
    "│   │       └── pc1_pc2.png\n",
    "│   ...\n",
    "├── ln2_pca_histograms\n",
    "│   ├── layer_0\n",
    "│   │   └── pc1_pc2.png\n",
    "│   ...\n",
    "├── ln2_sae_effect_err_pca_histograms\n",
    "│   ├── layer_0\n",
    "│   │   ├── normalized_error_norm\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── normalized_mse\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── normalized_variance_explained\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── unnormalized_error_norm\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── unnormalized_mse\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   └── unnormalized_variance_explained\n",
    "│   │       └── pc1_pc2.png\n",
    "│   ...\n",
    "├── ln2_sae_effect_pca_histograms\n",
    "│   ├── layer_0\n",
    "│   │   └── pc1_pc2.png\n",
    "│   ├── layer_1\n",
    "│   │   └── pc1_pc2.png\n",
    "│   ...\n",
    "├── ln2_sae_err_norms\n",
    "│   ├── layer_0.json\n",
    "│   ├── layer_0.png\n",
    "│   ...\n",
    "├── ln2_sae_mse\n",
    "│   ├── layer_0.json\n",
    "│   ├── layer_0.png\n",
    "│   ...\n",
    "├── ln2_sae_variance_explained\n",
    "│   ├── layer_0.json\n",
    "│   ├── layer_0.png\n",
    "│   ...\n",
    "├── plots_cosine_sim_pca_post_sae\n",
    "│   ├── layer_0.png\n",
    "│   ├── layer_0_sae_ins_ln2.png\n",
    "│   ...\n",
    "├── res_sae_err_norms\n",
    "│   ├── layer_0.json\n",
    "│   ├── layer_0.png\n",
    "│   ...\n",
    "├── res_sae_in_err_pca_histograms\n",
    "│   ├── layer_0\n",
    "│   │   ├── normalized_error_norm\n",
    "│   │   │   └── pc1_pc2.png\n",
    "|   |   |   ... (there might be more pairs of PCs)\n",
    "│   │   ├── normalized_mse\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── normalized_variance_explained\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── unnormalized_error_norm\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── unnormalized_mse\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   └── unnormalized_variance_explained\n",
    "│   │       └── pc1_pc2.png\n",
    "│   ...\n",
    "├── res_sae_in_pca_histograms\n",
    "│   ├── layer_0\n",
    "│   │   └── pc1_pc2.png\n",
    "│   ├── layer_1\n",
    "│   │   └── pc1_pc2.png\n",
    "│   ...\n",
    "├── res_sae_mse\n",
    "│   ├── layer_0.json\n",
    "│   ├── layer_0.png\n",
    "│   ├── ...\n",
    "|   ...\n",
    "├── res_sae_out_err_pca_histograms\n",
    "│   ├── layer_0\n",
    "│   │   ├── normalized_error_norm\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── normalized_mse\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── normalized_variance_explained\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   ├── unnormalized_error_norm\n",
    "│   │   │   └── pc1_pc2.png\n",
    "|   |   |   ... (there might be more pairs of PCs)\n",
    "│   │   ├── unnormalized_mse\n",
    "│   │   │   └── pc1_pc2.png\n",
    "│   │   └── unnormalized_variance_explained\n",
    "│   │       └── pc1_pc2.png\n",
    "│   ├── layer_1\n",
    "│   │   ├── normalized_error_norm\n",
    "│   │   ...\n",
    "|   ...\n",
    "├── res_sae_out_pca_histograms\n",
    "│   ├── layer_0\n",
    "│   │   └── pc1_pc2.png\n",
    "│   ├── layer_1\n",
    "│   │   └── pc1_pc2.png\n",
    "│   ...\n",
    "└── res_sae_variance_explained\n",
    "    ├── layer_0.json\n",
    "    ├── layer_0.png\n",
    "    ├── ...\n",
    "    ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-density",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
