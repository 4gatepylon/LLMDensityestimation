{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe point of this here is basically to try using the `delphi` library from EleutherAI\\n(https://github.com/4gatepylon/delphi) to do autointerp and then modify it/call it\\nin such a way as to do iterative improvement for the autointerp.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\"\"\"\n",
    "The point of this here is basically to try using the `delphi` library from EleutherAI\n",
    "(https://github.com/4gatepylon/delphi) to do autointerp and then modify it/call it\n",
    "in such a way as to do iterative improvement for the autointerp.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³  Loading Gemma-2B â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.82s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³  Loading Gemma-Scope SAE â€¦\n",
      "â³  Caching latent activations (â‰ˆ100 k tokens) â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 721/721 [00:00<00:00, 20.7kB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [10:25<00:00,  6.45s/files]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9508400/9508400 [03:00<00:00, 52712.46 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized instruction format: train[:0.05%]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msparsify\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chunk_and_tokenize\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdelphi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlatents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LatentCache                 \u001b[38;5;66;03m# :contentReference[oaicite:0]{index=0}\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m raw_text = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEleutherAI/fineweb-edu-dedup-10b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m                        \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain[:0.05\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m tokens = chunk_and_tokenize(raw_text,\n\u001b[32m     58\u001b[39m                             tokenizer,\n\u001b[32m     59\u001b[39m                             max_seq_len=\u001b[32m256\u001b[39m,\n\u001b[32m     60\u001b[39m                             text_key=\u001b[33m\"\u001b[39m\u001b[33mraw_content\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     62\u001b[39m cache = LatentCache(model, submodule_dict, batch_size=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/load.py:2640\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2636\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2637\u001b[39m keep_in_memory = (\n\u001b[32m   2638\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2639\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2640\u001b[39m ds = \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[32m   2642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2643\u001b[39m     \u001b[38;5;66;03m# To avoid issuing the same warning twice\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/builder.py:1268\u001b[39m, in \u001b[36mDatasetBuilder.as_dataset\u001b[39m\u001b[34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[39m\n\u001b[32m   1265\u001b[39m verification_mode = VerificationMode(verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS)\n\u001b[32m   1267\u001b[39m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m datasets = \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_single_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m   1280\u001b[39m     datasets = DatasetDict(datasets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/utils/py_utils.py:484\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    483\u001b[39m     data_struct = [data_struct]\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m mapped = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    486\u001b[39m     mapped = mapped[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/builder.py:1298\u001b[39m, in \u001b[36mDatasetBuilder._build_single_dataset\u001b[39m\u001b[34m(self, split, run_post_process, verification_mode, in_memory)\u001b[39m\n\u001b[32m   1295\u001b[39m     split = Split(split)\n\u001b[32m   1297\u001b[39m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m ds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_post_process:\n\u001b[32m   1303\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m resource_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post_processing_resources(split).values():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/builder.py:1372\u001b[39m, in \u001b[36mDatasetBuilder._as_dataset\u001b[39m\u001b[34m(self, split, in_memory)\u001b[39m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_legacy_cache():\n\u001b[32m   1371\u001b[39m     dataset_name = \u001b[38;5;28mself\u001b[39m.name\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m dataset_kwargs = \u001b[43mArrowReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m fingerprint = \u001b[38;5;28mself\u001b[39m._get_dataset_fingerprint(split)\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(fingerprint=fingerprint, **dataset_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/arrow_reader.py:252\u001b[39m, in \u001b[36mBaseReader.read\u001b[39m\u001b[34m(self, name, instructions, split_infos, in_memory)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    233\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     in_memory=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    237\u001b[39m ):\n\u001b[32m    238\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns Dataset instance(s).\u001b[39;00m\n\u001b[32m    239\u001b[39m \n\u001b[32m    240\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    249\u001b[39m \u001b[33;03m         kwargs to build a single Dataset instance.\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     files = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_file_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[32m    254\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInstruction \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m corresponds to no data!\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/arrow_reader.py:225\u001b[39m, in \u001b[36mBaseReader.get_file_instructions\u001b[39m\u001b[34m(self, name, instruction, split_infos)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_file_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, instruction, split_infos):\n\u001b[32m    224\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     file_instructions = \u001b[43mmake_file_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiletype_suffix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filetype_suffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     files = file_instructions.file_instructions\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/arrow_reader.py:132\u001b[39m, in \u001b[36mmake_file_instructions\u001b[39m\u001b[34m(name, split_infos, instruction, filetype_suffix, prefix_path)\u001b[39m\n\u001b[32m    121\u001b[39m name2filenames = {\n\u001b[32m    122\u001b[39m     info.name: filenames_for_dataset_split(\n\u001b[32m    123\u001b[39m         path=prefix_path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m split_infos\n\u001b[32m    130\u001b[39m }\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instruction, ReadInstruction):\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     instruction = \u001b[43mReadInstruction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[32m    134\u001b[39m absolute_instructions = instruction.to_absolute(name2len)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/arrow_reader.py:607\u001b[39m, in \u001b[36mReadInstruction.from_spec\u001b[39m\u001b[34m(cls, spec)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subs:\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo instructions could be built out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m instruction = \u001b[43m_str_to_read_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m((_str_to_read_instruction(sub) \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m subs[\u001b[32m1\u001b[39m:]), instruction)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/datasets/arrow_reader.py:444\u001b[39m, in \u001b[36m_str_to_read_instruction\u001b[39m\u001b[34m(spec)\u001b[39m\n\u001b[32m    442\u001b[39m res = _SUB_SPEC_RE.match(spec)\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized instruction format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    445\u001b[39m unit = \u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res.group(\u001b[33m\"\u001b[39m\u001b[33mfrom_pct\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m res.group(\u001b[33m\"\u001b[39m\u001b[33mto_pct\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mabs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ReadInstruction(\n\u001b[32m    447\u001b[39m     split_name=res.group(\u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    448\u001b[39m     rounding=res.group(\u001b[33m\"\u001b[39m\u001b[33mrounding\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    451\u001b[39m     unit=unit,\n\u001b[32m    452\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized instruction format: train[:0.05%]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "quick_gemma_scope_interpret.py\n",
    "--------------------------------\n",
    "A concise Delphi demo for interpreting the first 100 features of a\n",
    "Gemma-Scope sparse auto-encoder (SAE).\n",
    "\n",
    "Requirements (â‰ˆ the main ones)\n",
    "pip install \"torch>=2.2\" transformers sae-lens delphi sparsify datasets orjson pydantic\n",
    "\"\"\"\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import orjson\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1.  Choose the model + SAE you want to probe\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_MODEL = \"google/gemma-2b\"\n",
    "SAE_RELEASE = \"gemma-scope-2b-pt-res-canonical\"          # HuggingFace repo\n",
    "SAE_ID      = \"layer_10/width_16k/canonical\"             # particular layer\n",
    "HOOKPOINT   = \"layers.10\"                                # module name in Gemma\n",
    "MAX_LATENTS = 100                                        # how many features\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2.  Load Gemma and plug in the SAE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â³  Loading Gemma-2B â€¦\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "model     = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"â³  Loading Gemma-Scope SAE â€¦\")\n",
    "from sae_lens import SAE                              # pip install sae-lens\n",
    "\n",
    "sae, *_ = SAE.from_pretrained(release=SAE_RELEASE, sae_id=SAE_ID)\n",
    "\n",
    "# Map the model hook-point to the SAE you just loaded\n",
    "submodule_dict = {HOOKPOINT: sae}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3.  Cache a *tiny* latent dataset (demo-sized on purpose)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â³  Caching latent activations (â‰ˆ100 k tokens) â€¦\")\n",
    "from sparsify.data import chunk_and_tokenize\n",
    "from delphi.latents import LatentCache                 # :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "raw_text = load_dataset(\"EleutherAI/fineweb-edu-dedup-10b\",\n",
    "                        split=\"train[:0.05%]\")\n",
    "tokens = chunk_and_tokenize(raw_text,\n",
    "                            tokenizer,\n",
    "                            max_seq_len=256,\n",
    "                            text_key=\"raw_content\")[\"input_ids\"]\n",
    "\n",
    "cache = LatentCache(model, submodule_dict, batch_size=4)\n",
    "cache.run(n_tokens=100_000, tokens=tokens)\n",
    "cache.save_splits(n_splits=1, save_dir=\"latents\")       # creates ./latents/*.safetensors\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4.  Build a LatentDataset targeting the first 100 features\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from delphi.latents import LatentDataset\n",
    "from delphi.config  import SamplerConfig, ConstructorConfig\n",
    "\n",
    "latents = {HOOKPOINT: torch.arange(MAX_LATENTS)}        # [0 â€¦ 99]\n",
    "dataset = LatentDataset(\n",
    "    raw_dir=\"latents\",\n",
    "    modules=[HOOKPOINT],\n",
    "    latents=latents,\n",
    "    sampler_cfg=SamplerConfig(),\n",
    "    constructor_cfg=ConstructorConfig(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5.  Configure Delphiâ€™s explainer (local Llama-3-Instruct via vLLM)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TODO(Adrianoh) we probably want to switch to OpenAI/Anthropic's API?\n",
    "from delphi.clients   import Offline\n",
    "from delphi.explainers import DefaultExplainer\n",
    "from delphi.pipeline  import Pipeline, process_wrapper\n",
    "\n",
    "client    = Offline(\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                    num_gpus=1,                       # tweak to match your GPU(s)\n",
    "                    max_memory=0.90,\n",
    "                    max_model_len=8192)\n",
    "\n",
    "explainer = DefaultExplainer(client, tokenizer=tokenizer)\n",
    "explainer_pipe = process_wrapper(explainer)\n",
    "\n",
    "pipeline = Pipeline(dataset, explainer_pipe)\n",
    "\n",
    "print(\"ğŸ§   Generating explanations â€¦ (this can take ~minutes-hours)\")\n",
    "asyncio.run(pipeline.run(n_processes=4))                # CPU-bound; bump if you wish\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6.  Shape the results with a Pydantic schema\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class LatentExplanation(BaseModel):\n",
    "    latent_id: int\n",
    "    description: str\n",
    "    positive_examples: List[str]\n",
    "    negative_examples: List[str]\n",
    "\n",
    "out: List[LatentExplanation] = []\n",
    "\n",
    "for record in dataset:          # after the pipeline each record now has .explanation\n",
    "    exp = record.explanation\n",
    "    out.append(\n",
    "        LatentExplanation(\n",
    "            latent_id   = int(record.latent),\n",
    "            description = exp[\"explanation\"],\n",
    "            positive_examples = exp.get(\"top_activating_tokens\", []),\n",
    "            negative_examples = exp.get(\"most_confused_tokens\", [])\n",
    "        )\n",
    "    )\n",
    "\n",
    "dest = Path(\"gemma_scope_layer10_first100_explanations.json\")\n",
    "dest.write_bytes(orjson.dumps([e.dict() for e in out], option=orjson.OPT_INDENT_2))\n",
    "print(f\"âœ…  Saved structured explanations â†’ {dest.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-density",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
