{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "\n",
    "# Load a model (eg GPT-2 Small)\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "\n",
    "# Run the model and get logits and activations\n",
    "logits, activations = model.run_with_cache(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Skylion007/openwebtext\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lengths = np.array([len(x[\"text\"]) for x in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of lengths:  4914.9018131169\n",
      "std of lengths:  6662.991617660633\n",
      "min of lengths:  316\n",
      "max of lengths:  100000\n",
      "total lengths:  39386887788\n"
     ]
    }
   ],
   "source": [
    "print(\"mean of lengths: \", np.mean(lengths))\n",
    "print(\"std of lengths: \", np.std(lengths))\n",
    "print(\"min of lengths: \", np.min(lengths))\n",
    "print(\"max of lengths: \", np.max(lengths))\n",
    "print(\"total lengths: \", np.sum(lengths)) # roughly 3x tokens => 13B tokens first order magnitude\n",
    "# around 13B * 100 dims * 2 bytes => 2.5PB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple\n",
    "from jaxtyping import Float\n",
    "\n",
    "class NaiveNormalizingFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    Very naive normalizing flow model using the very first model introduced in this\n",
    "    blogpost: https://lilianweng.github.io/posts/2018-10-13-flow-models/.\n",
    "\n",
    "    It works like this:\n",
    "        - You have a sequence of shifts and scales\n",
    "        - Each subsequent element shifts the second shift scales set of indices\n",
    "            by the shift of the first layer and scales by the scale of the\n",
    "            first one too.\n",
    "    NOTE: does not support general index mask (TODO you might want to implement this?)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            shift_scales: List[Tuple[nn.Module, nn.Module]],\n",
    "            split_index: List[Tuple[int, bool]]\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.shift_scales = nn.ModuleList(shift_scales)\n",
    "        # NOTE: we train a bunch of linears, but they must all be invertible.\n",
    "        # TODO(Adriano) add linear layers\n",
    "        # self.linears = nn.ModuleList([nn.Linear(dim, dim, bias=True) for _ in range(len(shift_scales))])\n",
    "        self.split_index = split_index\n",
    "        assert len(split_index) == len(shift_scales)\n",
    "        assert all(sum(ratio) == dim for ratio in split_index)\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"batch dim\"]) -> Float[torch.Tensor, \"batch dim\"]:\n",
    "        \"\"\"\n",
    "        Implement sequential shift and scaling, alternating by index.\n",
    "        \"\"\"\n",
    "        for (shift, scale), (index, index_is_upper) in zip(self.shift_scales, self.split_index):\n",
    "            assert x.ndim == 2, f\"x.shape={x.shape} (you should just flatten into ndim=2 into batch dim)\"\n",
    "            assert x.shape[1] == self.dim\n",
    "\n",
    "            # Calculate the shift and scale\n",
    "            sh = shift(x[..., :index]) if index_is_upper else shift(x[..., index:])\n",
    "            assert sh.ndim == 2\n",
    "            assert sh.shape[1] == self.dim - index if index_is_upper else index # Need to fit in REST of indices; fmt: skip\n",
    "            # ...\n",
    "            sc = scale(x[..., :index]) if index_is_upper else scale(x[..., index:])\n",
    "            assert sc.ndim == 2\n",
    "            assert sc.shape[1] == self.dim - index if index_is_upper else index # Need to fit in REST of indices; fmt: skip\n",
    "\n",
    "            # Apply the scale and shift\n",
    "            if index_is_upper: # Means that you READ from BELOW index and WRITE ABOVE index\n",
    "                x[..., index:] = x[..., index:] * sc.exp()\n",
    "                x[..., index:] = x[..., index:] + sh\n",
    "            else: # Means that READ from ABOVE index and WRITE BELOW index\n",
    "                x[..., :index] = x[..., :index] * sc.exp()\n",
    "                x[..., :index] = x[..., :index] + sh\n",
    "        return x\n",
    "    \n",
    "    # TODO(Adriano) implement this\n",
    "    # def inverse(y: Float[\"batch dim\", torch.Tensor]) -> Float[\"batch dim\", torch.Tensor]:\n",
    "    #     raise NotImplementedError(\"Not implemented\")\n",
    "\n",
    "    # def log_det_jacobians(self, x: Float[\"batch dim\", torch.Tensor]) -> Float[\"batch dim\", torch.Tensor]:\n",
    "    #     raise NotImplementedError(\"Not implemented\")\n",
    "    # def log_det_jacobian(self, x: Float[\"batch dim\", torch.Tensor]) -> Float[\"batch dim\", torch.Tensor]:\n",
    "    #     return torch.sum(self.log_det_jacobians(x), dim=-1)\n",
    "    \n",
    "    # def log_prob(self, x: Float[\"batch dim\", torch.Tensor]) -> Float[\"batch dim\", torch.Tensor]:\n",
    "    #     raise NotImplementedError(\"Not implemented\")\n",
    "    \n",
    "    # def sample(self, n: int) -> Float[\"batch dim\", torch.Tensor]:\n",
    "    #     raise NotImplementedError(\"Not implemented\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m mlps[\u001b[32m0\u001b[39m](torch.randn(\u001b[32m10\u001b[39m, \u001b[32m64\u001b[39m)) \u001b[38;5;66;03m# Make sure no error\u001b[39;00m\n\u001b[32m     15\u001b[39m mlps[\u001b[32m1\u001b[39m](torch.randn(\u001b[32m10\u001b[39m, \u001b[32m64\u001b[39m)) \u001b[38;5;66;03m# Make sure no error\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m flow = \u001b[43mNaiveNormalizingFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m x = torch.randn(batch_size, dim)\n\u001b[32m     20\u001b[39m y = flow(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mNaiveNormalizingFlow.__init__\u001b[39m\u001b[34m(self, dim, shift_scales, split_index)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mself\u001b[39m.split_index = split_index\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(split_index) == \u001b[38;5;28mlen\u001b[39m(shift_scales)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28msum\u001b[39m(ratio) == dim \u001b[38;5;28;01mfor\u001b[39;00m ratio \u001b[38;5;129;01min\u001b[39;00m split_index)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "################ Test that the flow forwards method works (i.e. to give you output) #################\n",
    "batch_size = 100\n",
    "dim = 128\n",
    "split_index = [(64, True), (64, False)]\n",
    "mlps = [nn.Sequential(\n",
    "    nn.Linear(d if u else dim - d, d if u else dim - d),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(d if u else dim - d, d if u else dim - d),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(d if u else dim - d, d if u else dim - d),\n",
    ") for d, u in split_index]\n",
    "assert len(mlps) == len(split_index)\n",
    "\n",
    "mlps[0](torch.randn(10, 64)) # Make sure no error\n",
    "mlps[1](torch.randn(10, 64)) # Make sure no error\n",
    "\n",
    "flow = NaiveNormalizingFlow(dim, mlps, split_index)\n",
    "\n",
    "x = torch.randn(batch_size, dim)\n",
    "y = flow(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average loss is: 4.412842750549316: 100%|██████████| 1/1 [00:00<00:00, 37.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs has shape: torch.Size([1, 1, 21, 768])\n",
      "Losses has shape torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Collect some activations :/\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from collect_activations import collect_all_activations\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# hf_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "# model = HookedTransformer.from_pretrained('google/gemma-2-9b-it', device=device)\n",
    "# text = \"bob is in paris, alice is in tokyo, actually bob is in london... bob is in\"\n",
    "# text_tok = hf_tokenizer(text, return_tensors = \"pt\").to(device)[\"input_ids\"]\n",
    "# # print(message_tokens)\n",
    "# response = model.generate(text_tok, max_new_tokens = 8)\n",
    "# print(hf_tokenizer.decode(response[0], skip_special_tokens = True))\n",
    "# model = model.to(device)\n",
    "# text = dataset[0][\"text\"]\n",
    "# text_tok = model.to_tokens(text)\n",
    "# model(text_tok)\n",
    "# Test that we are able to collet some activations\n",
    "test_hook_names = [\"blocks.6.hook_resid_pre\"]\n",
    "test_msg = \"I am a happy watermelon. Every day I roll roll roll down the hill to see my friend the\"\n",
    "test_tok = model.tokenizer(test_msg, return_tensors = \"pt\").to(device)[\"input_ids\"]\n",
    "# test_resp_tok = model.generate(test_tok, max_new_tokens = 32)\n",
    "# test_resp = hf_tokenizer.decode(test_resp_tok[0], skip_special_tokens = True)\n",
    "# # debug\n",
    "# test_brk = \"\\n\" + \"=\"*100 + \"\\n\"\n",
    "# print(test_msg, test_brk, test_tok, test_brk, test_resp_tok, test_brk, test_resp)\n",
    "\n",
    "# run em\n",
    "outputs, losses = collect_all_activations(model, test_tok, test_hook_names, inference_batch_size=20)\n",
    "print(f\"Outputs has shape: {outputs.shape}\")\n",
    "print(f\"Losses has shape {losses.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-density",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
