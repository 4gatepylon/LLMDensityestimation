{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sae_lens import SAE  # pip install sae-lens\n",
    "import torch\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple, Optional\n",
    "import tqdm\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_sae(model_name: str = \"google/gemma-2-2b\", layer: int = 20):\n",
    "    assert model_name in [\"gpt2\", \"google/gemma-2-2b\"]\n",
    "    with torch.no_grad():\n",
    "        # 1. Fetch the SAE\n",
    "        assert model_name in [\"gpt2\", \"google/gemma-2-2b\"]\n",
    "        if model_name == \"gpt2\":\n",
    "            sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "                    release = \"gpt2-small-res-jb\",\n",
    "                    sae_id = f\"blocks.{layer}.hook_resid_pre\",\n",
    "                    device = \"cuda\"\n",
    "                )\n",
    "        else:\n",
    "            sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "                release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "                sae_id=f\"layer_{layer}/width_16k/canonical\",\n",
    "            )\n",
    "        sae.cuda()\n",
    "        sae.eval()\n",
    "        for p in sae.parameters():\n",
    "            p.requires_grad = False\n",
    "            p.grad = None\n",
    "        return sae\n",
    "def get_model(model_name: str = \"google/gemma-2-2b\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\")\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "        p.grad = None\n",
    "    return model\n",
    "    \n",
    "def get_tokenizer(model_name: str = \"google/gemma-2-2b\"):\n",
    "    return AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_dataset(dataset_name: str = \"stas/openwebtext-10k\", batch_size: Optional[int] = None, tokenizer: Optional[AutoTokenizer] = None):\n",
    "    if tokenizer is None:\n",
    "        raise ValueError(\"Tokenizer is required for `get_dataset`\")\n",
    "    dataset_name = \"stas/openwebtext-10k\"  # yolo\n",
    "    dataset = load_dataset(\n",
    "        dataset_name, split=\"train\", trust_remote_code=True\n",
    "    )  # Smaller version\n",
    "    tokens = tokenize_and_concatenate(\n",
    "        dataset=dataset,  # type: ignore\n",
    "        tokenizer=tokenizer,  # type: ignore\n",
    "        streaming=True,\n",
    "        # NOTE: all these have context 128\n",
    "        max_length=128,  # sae.cfg.context_size,\n",
    "        add_bos_token=True,  # sae.cfg.prepend_bos,\n",
    "    )[\"tokens\"]\n",
    "    tokens = tokens.to(\"cuda\")  # eh\n",
    "    if batch_size is not None:\n",
    "        tokens = tokens[:batch_size]\n",
    "    return tokens\n",
    "\n",
    "def get_activations(model_name: str = \"google/gemma-2-2b\", layer: int = 20):\n",
    "    torch.set_grad_enabled(False)\n",
    "    with torch.no_grad():\n",
    "        \"\"\"\n",
    "        Load the things...\n",
    "        \"\"\"\n",
    "        # 1. Get our model tokenizer etc...\n",
    "        # sae = get_sae(model_name, layer) # Not used yet lmao\n",
    "        model = get_model(model_name)\n",
    "        tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "        # GEt the full dataset\n",
    "        dataset_name = \"stas/openwebtext-10k\"  # yolo\n",
    "        n_batch = 10_000\n",
    "        tokens = get_dataset(dataset_name, batch_size=n_batch, tokenizer=tokenizer)\n",
    "\n",
    "    \"\"\"\n",
    "    Collect some output activations.\n",
    "    \"\"\"\n",
    "    collected_outputs = []\n",
    "    def gather_target_act_hook(mod, inputs, outputs):\n",
    "        nonlocal collected_outputs\n",
    "        assert isinstance(outputs, tuple)\n",
    "        assert isinstance(outputs[0], torch.Tensor), f\"Expected a single tensor output, got {outputs}\" # fmt: skip\n",
    "        collected_outputs.append(outputs[0].detach().requires_grad_(False).cpu())\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    if model_name == \"gpt2\":\n",
    "        handle = model.transformer.h[layer].register_forward_hook(gather_target_act_hook)\n",
    "    else:\n",
    "        handle = model.model.layers[layer].register_forward_hook(gather_target_act_hook)\n",
    "    try:\n",
    "        batch_size = 100\n",
    "        for i in tqdm.trange(0, tokens.shape[0], batch_size):\n",
    "            j = min(i + batch_size, tokens.shape[0])\n",
    "            model.forward(tokens[i:j])\n",
    "    finally:\n",
    "        handle.remove()\n",
    "    collected_outputs = torch.cat(collected_outputs, dim=0)\n",
    "    print(collected_outputs.shape)\n",
    "\n",
    "    tokens_is_special = (\n",
    "        (tokens == tokenizer.bos_token_id)\n",
    "        | (tokens == tokenizer.eos_token_id)\n",
    "        | (tokens == tokenizer.pad_token_id)\n",
    "    )\n",
    "    tokens_is_special_flat = tokens_is_special.cpu().flatten()\n",
    "    collected_outputs_flat = collected_outputs.cpu().reshape(\n",
    "        -1, collected_outputs.shape[-1]\n",
    "    )\n",
    "    activations = collected_outputs_flat[~tokens_is_special_flat, :]\n",
    "    print(activations.shape)  # These are the ones we will use to understand our SAE\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi shape: torch.Size([1024, 2145]) from bias=1 plus quadratic=2080 + linear=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting probe over batches (batch_size=1024): 100%|██████████| 1/1 [00:00<00:00, 45.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.9999999999981426\n",
      "R² batched: 0.9999999999981426\n",
      "R² on linear targets: 0.9999999999987129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing R² over batches (batch_size=1024): 100%|██████████| 1/1 [00:00<00:00, 620.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² batched on linear targets: 0.9999999999987129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Callable, Optional\n",
    "\n",
    "class QuadraticFeatureMap:\n",
    "    \"\"\"Second‑order (quadratic) feature expansion.\n",
    "\n",
    "    Given activations X ∈ ℝ^{N×d}, returns Φ(X) that concatenates an optional bias,\n",
    "    the original linear terms, and the unique quadratic terms x_i x_j with i ≤ j.\n",
    "    Optionally subsamples quadratic terms to keep dimensionality manageable.\n",
    "\n",
    "    By o3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        include_bias: bool = True,\n",
    "        include_linear: bool = True,\n",
    "        max_quadratic_features: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        self.include_bias = include_bias\n",
    "        self.include_linear = include_linear\n",
    "        self.max_quadratic_features = max_quadratic_features\n",
    "        self._tri_idx_cache = {}\n",
    "\n",
    "    def _upper_tri_indices(self, d: int, device: torch.device):\n",
    "        # Cache indices so we don't re‑allocate on every call\n",
    "        if (d, device) not in self._tri_idx_cache:\n",
    "            self._tri_idx_cache[(d, device)] = torch.triu_indices(d, d, device=device)\n",
    "        return self._tri_idx_cache[(d, device)]\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Compute the quadratic feature map.\n",
    "\n",
    "        Args:\n",
    "            x: (N, d) activations.\n",
    "        Returns:\n",
    "            Φ(x): (N, D) transformed feature matrix.\n",
    "        \"\"\"\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Input must have shape (N, d)\")\n",
    "        N, d = x.shape\n",
    "        parts = []\n",
    "        if self.include_bias:\n",
    "            parts.append(torch.ones(N, 1, device=x.device, dtype=x.dtype))\n",
    "        if self.include_linear:\n",
    "            parts.append(x)\n",
    "\n",
    "        # Quadratic terms – only keep i ≤ j to avoid duplicates\n",
    "        tri_i, tri_j = self._upper_tri_indices(d, x.device)\n",
    "        quad_terms = x.unsqueeze(2) * x.unsqueeze(1)  # (N, d, d)\n",
    "        quad_terms = quad_terms[:, tri_i, tri_j]      # (N, d(d+1)/2)\n",
    "\n",
    "        if self.max_quadratic_features is not None and quad_terms.shape[1] > self.max_quadratic_features:\n",
    "            # Uniform random subsample to cap dimensionality\n",
    "            idx = torch.randperm(quad_terms.shape[1], device=x.device)[: self.max_quadratic_features]\n",
    "            quad_terms = quad_terms[:, idx]\n",
    "        parts.append(quad_terms)\n",
    "        return torch.cat(parts, dim=1)\n",
    "\n",
    "\n",
    "class NonLinearProbe(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Generic non‑linear probe: Φ(·) → linear ridge regression.\n",
    "\n",
    "    By o3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_map: Callable[[Tensor], Tensor],\n",
    "        reg_lambda: float = 1e-4,\n",
    "        device: torch.device | str = \"cpu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_map = feature_map\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.device = torch.device(device)\n",
    "        self.weight: Optional[Tensor] = None  # (D, k)\n",
    "\n",
    "    def fit(self, X: Tensor, y: Tensor, batch_size: Optional[int] = None) -> None:\n",
    "        \"\"\"Fit ridge regression weights.\n",
    "\n",
    "        Args:\n",
    "            X: (N, d) activations.\n",
    "            y: (N, k) targets (e.g., SAE residuals).\n",
    "        \"\"\"\n",
    "        if batch_size is not None:\n",
    "            return self.fit_batched(X, y, batch_size)\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        Φ = self.feature_map(X)  # (N, D)\n",
    "        # Closed‑form ridge solution: W = (ΦᵀΦ + λI)^{-1} Φᵀ y\n",
    "        XtX = Φ.T @ Φ\n",
    "        if self.reg_lambda > 0:\n",
    "            XtX += self.reg_lambda * torch.eye(XtX.size(0), device=self.device, dtype=Φ.dtype)\n",
    "        self.weight = torch.linalg.solve(XtX, Φ.T @ y)\n",
    "    \n",
    "    def fit_batched(self, X: Tensor, y: Tensor, batch_size: int = 1024) -> None:\n",
    "        \"\"\"Fit ridge regression weights. By Claude.\n",
    "\n",
    "        Args:\n",
    "            X: (N, d) activations.\n",
    "            y: (N, k) targets (e.g., SAE residuals).\n",
    "            batch_size: int, batch size for processing large datasets.\n",
    "        \"\"\"\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        XtX = None\n",
    "        XtY = None\n",
    "        \n",
    "        for i in tqdm.trange(0, N, batch_size, desc=f\"Fitting probe over batches (batch_size={batch_size})\"):\n",
    "            end_idx = min(i + batch_size, N)\n",
    "            X_batch = X[i:end_idx]\n",
    "            y_batch = y[i:end_idx]\n",
    "            \n",
    "            # Transform batch\n",
    "            Φ_batch = self.feature_map(X_batch)  # (batch_size, D)\n",
    "            \n",
    "            # Accumulate statistics\n",
    "            batch_XtX = Φ_batch.T @ Φ_batch\n",
    "            batch_XtY = Φ_batch.T @ y_batch\n",
    "            \n",
    "            if XtX is None:\n",
    "                XtX = batch_XtX\n",
    "                XtY = batch_XtY\n",
    "            else:\n",
    "                XtX += batch_XtX\n",
    "                XtY += batch_XtY\n",
    "        \n",
    "        # Closed‑form ridge solution: W = (ΦᵀΦ + λI)^{-1} Φᵀ y\n",
    "        if self.reg_lambda > 0:\n",
    "            XtX += self.reg_lambda * torch.eye(XtX.size(0), device=self.device, dtype=XtX.dtype)\n",
    "        \n",
    "        self.weight = torch.linalg.solve(XtX, XtY)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, X: Tensor) -> Tensor:\n",
    "        if self.weight is None:\n",
    "            raise RuntimeError(\"Probe has not been fitted yet.\")\n",
    "        Φ = self.feature_map(X.to(self.device))\n",
    "        return Φ @ self.weight  # (N, k)\n",
    "\n",
    "    # TODO(Adriano) not entirely sure this will be numerically stable ngl\n",
    "    @torch.no_grad()\n",
    "    def r2(self, X: Tensor, y: Tensor, batch_size: Optional[int] = None) -> float:\n",
    "        if batch_size is not None:\n",
    "            return self.r2_batched(X, y, batch_size)\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = torch.sum((y.to(self.device) - y_pred) ** 2)\n",
    "        ss_tot = torch.sum((y.to(self.device) - y.mean(dim=0, keepdim=True).to(self.device)) ** 2)\n",
    "        return 1.0 - ss_res.item() / ss_tot.item()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def r2_batched(self, X: Tensor, y: Tensor, batch_size: int = 1024) -> float:\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        # Calculate y_mean for ss_tot\n",
    "        y_mean = y.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        ss_res = 0.0\n",
    "        ss_tot = 0.0\n",
    "        \n",
    "        for i in tqdm.trange(0, N, batch_size, desc=f\"Computing R² over batches (batch_size={batch_size})\"):\n",
    "            end_idx = min(i + batch_size, N)\n",
    "            X_batch = X[i:end_idx]\n",
    "            y_batch = y[i:end_idx]\n",
    "            \n",
    "            # Get predictions for this batch\n",
    "            y_pred_batch = self.predict(X_batch)\n",
    "            \n",
    "            # Accumulate sum of squared residuals\n",
    "            ss_res += torch.sum((y_batch - y_pred_batch) ** 2).item()\n",
    "            \n",
    "            # Accumulate total sum of squares\n",
    "            ss_tot += torch.sum((y_batch - y_mean) ** 2).item()\n",
    "        \n",
    "        return 1.0 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "# Example usage - DEBUG by o3\n",
    "torch.manual_seed(0)\n",
    "N, d, k = 1024, 64, 32\n",
    "X = torch.randn(N, d)\n",
    "true_W_quadratic = torch.randn(d + d * (d + 1) // 2 + 1, k) * 0.1  # bias + linear + quad\n",
    "true_W_linear = torch.randn(d + 1, k) * 0.1  # linear + bias\n",
    "# NOTE the first term of linear will be bias...\n",
    "true_W_linear_as_quadratic = torch.cat([true_W_linear, torch.zeros(d*(d+1)//2, k)], dim=0)\n",
    "fmap = QuadraticFeatureMap(include_bias=True, include_linear=True)\n",
    "Φ = fmap(X)\n",
    "print(\"phi shape:\", Φ.shape, f\"from bias={1} plus quadratic={d*(d+1)//2} + linear={d}\")\n",
    "y = Φ @ true_W_quadratic + 0.01 * torch.randn(N, k)  # synthetic targets\n",
    "y_linear = Φ @ true_W_linear_as_quadratic + 0.01 * torch.randn(N, k)  # synthetic targets\n",
    "\n",
    "probe = NonLinearProbe(fmap, reg_lambda=1e-3)\n",
    "probe_batched = NonLinearProbe(fmap, reg_lambda=1e-3)\n",
    "probe_on_linear = NonLinearProbe(fmap, reg_lambda=1e-3)\n",
    "probe.fit(X, y)\n",
    "probe_batched.fit_batched(X, y)\n",
    "probe_on_linear.fit(X, y_linear)\n",
    "print(\"R²:\", probe.r2(X, y))\n",
    "print(\"R² batched:\", probe_batched.r2(X, y))\n",
    "print(\"R² on linear targets:\", probe_on_linear.r2(X, y_linear)) # NOTE it should fit this too!\n",
    "print(\"R² batched on linear targets:\", probe_on_linear.r2_batched(X, y_linear))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear R²: 0.06380545902088575\n",
      "Linear R² on quadratic targets: 0.06379495898092746\n"
     ]
    }
   ],
   "source": [
    "# Create a linear feature map for comparison\n",
    "class LinearFeatureMap:\n",
    "    \"\"\"Linear feature expansion with optional bias.\n",
    "    \n",
    "    Given activations X ∈ ℝ^{N×d}, returns X with an optional bias term.\n",
    "\n",
    "    By Claude.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, include_bias: bool = True) -> None:\n",
    "        self.include_bias = include_bias\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Compute the linear feature map.\n",
    "        \n",
    "        Args:\n",
    "            x: (N, d) activations.\n",
    "        Returns:\n",
    "            Φ(x): (N, d+1) or (N, d) transformed feature matrix.\n",
    "        \"\"\"\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Input must have shape (N, d)\")\n",
    "        N, d = x.shape\n",
    "        \n",
    "        if self.include_bias:\n",
    "            bias = torch.ones(N, 1, device=x.device, dtype=x.dtype)\n",
    "            return torch.cat([bias, x], dim=1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "N, d, k = 1024, 64, 32\n",
    "X = torch.randn(N, d)\n",
    "true_W_linear = torch.randn(d + 1, k) * 0.1  # linear + bias\n",
    "linear_fmap = LinearFeatureMap()\n",
    "Φ_linear = linear_fmap(X)\n",
    "y_linear = Φ_linear @ true_W_linear + 0.01 * torch.randn(N, k)  # synthetic targets\n",
    "y_nonlinear = Φ @ true_W_quadratic + 0.01 * torch.randn(N, k)  # synthetic targets\n",
    "\n",
    "# Technically a LINEAR probe - show that it does not work on the quadratic case...\n",
    "linear_probe = NonLinearProbe(linear_fmap, reg_lambda=1e-3)\n",
    "linear_probe.fit(X, y)\n",
    "linear_probe_on_quad = NonLinearProbe(linear_fmap, reg_lambda=1e-3)\n",
    "linear_probe_on_quad.fit(X, y_nonlinear)\n",
    "linear_probe_on_quad.r2(X, y_nonlinear)\n",
    "print(\"Linear R²:\", linear_probe.r2(X, y))\n",
    "print(\"Linear R² on quadratic targets:\", linear_probe_on_quad.r2(X, y_nonlinear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 128, 768])\n",
      "torch.Size([1268818, 768])\n",
      "torch.Size([1268818, 768])\n"
     ]
    }
   ],
   "source": [
    "activations = get_activations(model_name=\"gpt2\", layer=6)\n",
    "print(activations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sae(\n",
    "    activations: Float[Tensor, \"n_samples n_features\"],\n",
    "    sae: SAE,\n",
    "    batch_size: int,\n",
    ") -> Float[Tensor, \"n_samples n_features\"]:\n",
    "    recons = []\n",
    "    activations_device = activations.device\n",
    "    sae_device = next(p.device for p in sae.parameters())\n",
    "    for i in tqdm.trange(\n",
    "        0, activations.shape[0], batch_size, desc=\"SAE forward (application)\"\n",
    "    ):\n",
    "        j = min(i + batch_size, activations.shape[0])\n",
    "        activations_batch = activations[i:j]\n",
    "        activations_batch = activations_batch.to(sae_device)\n",
    "        recons.append(sae(activations_batch).to(activations_device))\n",
    "    for r in recons:\n",
    "        r.cpu() # TODO(Adriano) fking OOM\n",
    "    recons_pt = torch.cat(recons, dim=0)\n",
    "    assert recons_pt.shape == activations.shape\n",
    "    return recons_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align3_drive/adrianoh/miniconda3/envs/llm-density/lib/python3.12/site-packages/sae_lens/sae.py:151: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n",
      "SAE forward (application): 100%|██████████| 1240/1240 [00:03<00:00, 402.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residuals shape: torch.Size([1268818, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting probe over batches (batch_size=15000): 100%|██████████| 85/85 [00:50<00:00,  1.70it/s]\n",
      "Computing R² over batches (batch_size=15000): 100%|██████████| 85/85 [00:09<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic probe held-out R²: 0.29060292904234497\n",
      "Linear probe held-out R²: 0.3715992409469595\n"
     ]
    }
   ],
   "source": [
    "# 1 · Get activations and SAE residuals (your existing pipeline).\n",
    "# NOTE: excludes BOS, EOS, etc...\n",
    "import gc\n",
    "try:\n",
    "    del acts\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del sae\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del recons\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del residuals\n",
    "except:\n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "########################################################################\n",
    "model_name = \"gpt2\"\n",
    "layer = 6\n",
    "acts = activations.detach().requires_grad_(False).cuda()  # (N, d)\n",
    "assert acts.ndim == 2, f\"acts.shape: {acts.shape}\"\n",
    "sae = get_sae(model_name, layer).cuda()                   # frozen SAE\n",
    "recons = apply_sae(acts, sae, batch_size=1024)            # (N, d)\n",
    "assert recons.ndim == 2, f\"recons.shape: {recons.shape}\"\n",
    "residuals = acts - recons                                 # (N, d)  ← target for the probe\n",
    "del recons # TODO(Adriano) avoid this if you want to predict from the result\n",
    "del sae\n",
    "########################################################################\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"residuals shape:\", residuals.shape)\n",
    "########################################################################\n",
    "\n",
    "# 2 · Instantiate probes (λ and feature cap are tunables).\n",
    "# Quadratic probe\n",
    "quad_fmap = QuadraticFeatureMap(max_quadratic_features=25_000)\n",
    "quad_probe = NonLinearProbe(quad_fmap, reg_lambda=1e-4, device=\"cuda\")\n",
    "\n",
    "# Linear probe for comparison\n",
    "linear_fmap = LinearFeatureMap(include_bias=True)\n",
    "linear_probe = NonLinearProbe(linear_fmap, reg_lambda=1e-4, device=\"cuda\")\n",
    "\n",
    "# 3 · Fit & evaluate both probes.\n",
    "quad_probe.fit(acts, residuals, batch_size=15_000)\n",
    "linear_probe.fit(acts, residuals)\n",
    "print(\"Quadratic probe held-out R²:\", quad_probe.r2(acts, residuals, batch_size=15_000)) # LOL IT SUCKS\n",
    "print(\"Linear probe held-out R²:\", linear_probe.r2(acts, residuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
